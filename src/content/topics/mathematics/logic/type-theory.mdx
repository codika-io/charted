---
title: "Type Theory"
description: "The modern frontier — propositions as types, proofs as programs, and the unification of logic with computation."
parent: mathematics/logic
order: 9
color: "#ef4444"
difficulty: advanced
prerequisites: ["mathematics/logic/proof-theory"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Type theory stands at the confluence of logic, computation, and mathematics, offering a single framework in which propositions are types, proofs are programs, and the act of simplifying a proof is literally the same as running a computation. Where the earlier topics in this branch built up formal languages, proof systems, models, and metatheoretic limits, type theory synthesizes these threads into something new: a foundation for mathematics that is simultaneously a programming language. It represents the culmination of ideas from proof theory, lambda calculus, and category theory — and in its most modern incarnation, homotopy type theory, it reaches into the heart of abstract geometry itself.

## Simple and Dependent Type Systems

The story of type theory begins with a paradox. In 1901, **Bertrand Russell** discovered that naive set theory — the idea that for any property $P$, there exists a set $\{x \mid P(x)\}$ — leads to contradiction. Consider the set $R = \{x \mid x \notin x\}$: does $R$ belong to itself? If it does, it does not; if it does not, it does. Russell's response, published in 1903 and refined in *Principia Mathematica* (1910-1913, with Whitehead), was to introduce a **ramified theory of types**: objects are arranged in a hierarchy of levels, and a set at level $n$ can only contain elements from level $n-1$. This prevents self-reference by construction. The idea was sound but cumbersome, requiring an additional "axiom of reducibility" that undermined much of its elegance.

The modern form of simple type theory was given by **Alonzo Church** in 1940. Church's system starts with a collection of **base types** — typically a type $o$ of propositions and a type $\iota$ of individuals — and builds new types using a single constructor: the **function type** $\sigma \to \tau$, representing functions from objects of type $\sigma$ to objects of type $\tau$. Every term in the language carries a type, and the fundamental judgment of the system is the **typing judgment**:

$$\Gamma \vdash t : \tau$$

which reads: "in the context $\Gamma$ (a list of variable-type assignments), the term $t$ has type $\tau$." This judgment is the spine of all type theories. Context $\Gamma$ might declare $x : \mathbb{N}$ and $f : \mathbb{N} \to \mathbb{N}$, and then we can derive $\Gamma \vdash f(x) : \mathbb{N}$. Simple type theory is already powerful enough to serve as a foundation for higher-order logic — Church's **simple theory of types** (also called higher-order logic or HOL) remains the basis for the Isabelle/HOL proof assistant to this day.

The leap from simple to **dependent type theory** is the leap from programming to proving. In a dependent type system, types can depend on values. A simple function type $A \to B$ says "given an input of type $A$, I produce an output of type $B$" — but $B$ is fixed in advance. A **dependent function type** $\Pi_{x:A} B(x)$ says "given an input $x$ of type $A$, I produce an output whose type $B(x)$ may depend on which particular $x$ was given." This is the type-theoretic counterpart of universal quantification $\forall x. P(x)$: a term of type $\Pi_{x:A} B(x)$ is a function that, for every $x : A$, produces a witness that $B(x)$ holds.

**Per Martin-Lof** introduced his **intuitionistic type theory** in 1972, providing the definitive framework for dependent types. Martin-Lof's system includes two fundamental type formers. The **$\Pi$-type** (dependent product) $\Pi_{x:A} B(x)$ generalizes function types: when $B$ does not depend on $x$, it reduces to the ordinary $A \to B$. The **$\Sigma$-type** (dependent sum) $\Sigma_{x:A} B(x)$ generalizes pairs: a term of this type is a pair $(a, b)$ where $a : A$ and $b : B(a)$, and the type of the second component depends on the value of the first. This is the type-theoretic counterpart of existential quantification $\exists x. P(x)$ — a proof is a witness $a$ together with evidence $b$ that $P(a)$ holds.

Martin-Lof's system also introduced **universes** — types whose elements are themselves types. A universe $\mathcal{U}_0$ is a "type of small types"; its elements include $\mathbb{N}$, $\text{Bool}$, and types built from them. To avoid paradoxes reminiscent of Russell's, universes are arranged in a hierarchy $\mathcal{U}_0 : \mathcal{U}_1 : \mathcal{U}_2 : \cdots$, where each universe lives in the next. The system is further enriched with **inductive types** — types defined by specifying their constructors, such as the natural numbers $\mathbb{N}$ with constructors $0 : \mathbb{N}$ and $\text{succ} : \mathbb{N} \to \mathbb{N}$, together with an elimination principle (the induction principle) that arises automatically from the definition. The difference between simple and dependent type theory is profound: simple types classify data, but dependent types classify evidence. In a dependent type theory, stating a theorem and constructing its proof are both activities within the same formal system.

## Lambda Calculus and Curry-Howard Correspondence

The computational engine at the heart of type theory is the **lambda calculus**, invented by **Alonzo Church** in the 1930s as a formalization of the notion of effective computability. In its purest form, the **untyped lambda calculus** has only three kinds of expression: variables $x$, abstractions $\lambda x. M$ (representing functions), and applications $M \; N$ (applying a function to an argument). Despite this extreme minimalism, Church showed that the lambda calculus can encode all computable functions — it is equivalent in power to Turing machines, a fact that is one pillar of the **Church-Turing thesis**.

The key computational rule is **$\beta$-reduction**: the expression $(\lambda x. M) \; N$ reduces to $M[N/x]$, the result of substituting $N$ for every free occurrence of $x$ in $M$. This is function application: if $\lambda x. x + 1$ represents the "add one" function, then $(\lambda x. x + 1) \; 3$ reduces to $3 + 1 = 4$. The **Church-Rosser theorem** (1936) guarantees that $\beta$-reduction is confluent: if a term can be reduced in two different ways, both reduction paths can be extended to reach the same result. This means the lambda calculus has a well-defined notion of "answer" — the normal form of an expression, if it exists, is unique.

The untyped lambda calculus is too wild for logic: some terms, like $\Omega = (\lambda x. x \; x)(\lambda x. x \; x)$, reduce to themselves forever and have no normal form. The **simply typed lambda calculus** (Church, 1940) tames this by assigning types to every term and rejecting ill-typed expressions. The crucial consequence is **strong normalization**: every well-typed term has a normal form, and every sequence of reductions terminates. No infinite loops, no divergence — computation always halts. This makes the simply typed lambda calculus too weak for general-purpose computation (it cannot express all computable functions) but exactly right for logic, where every proof should be finite.

The deepest insight connecting logic and computation is the **Curry-Howard correspondence**, discovered independently by **Haskell Curry** (1958) and **William Howard** (1969). The correspondence reveals that typed lambda calculus and natural deduction are not merely analogous — they are the same mathematical object viewed from two perspectives. Propositions are types; proofs are programs; and the process of simplifying a proof (cut-elimination in proof theory) is the same as running a program ($\beta$-reduction in lambda calculus).

The correspondence operates at the level of individual connectives. Each logical connective corresponds to a type constructor, and each inference rule corresponds to a term constructor:

| Logic | Type Theory | Proof/Program |
|---|---|---|
| $A \to B$ (implication) | $A \to B$ (function type) | $\lambda$-abstraction |
| $A \land B$ (conjunction) | $A \times B$ (product type) | pair $(a, b)$ |
| $A \lor B$ (disjunction) | $A + B$ (sum type) | injection $\text{inl}(a)$ or $\text{inr}(b)$ |
| $\forall x. P(x)$ (universal) | $\Pi_{x:A} B(x)$ (dependent function) | dependent function |
| $\exists x. P(x)$ (existential) | $\Sigma_{x:A} B(x)$ (dependent pair) | dependent pair $(a, b)$ |
| $\bot$ (falsity) | $\mathbf{0}$ (empty type) | no constructor |
| $\top$ (truth) | $\mathbf{1}$ (unit type) | trivial term $\star$ |

Under this reading, a proof of $A \to B$ is a function that transforms evidence for $A$ into evidence for $B$. A proof of $A \land B$ is a pair: evidence for $A$ together with evidence for $B$. A proof of $\forall x. P(x)$ is a function that, given any $x$, produces evidence that $P(x)$ holds. The correspondence even extends to proof normalization: Gentzen's cut-elimination procedure from proof theory corresponds exactly to $\beta$-reduction in the lambda calculus. When we "cut" a detour in a proof — an introduction rule immediately followed by an elimination rule — we are performing the same operation as evaluating a function application. This is one of the deepest connections in all of logic and computer science, and it forms the theoretical backbone of every modern proof assistant.

## Category-Theoretic Logic and Topoi

Category theory provides yet another lens through which to view logic, one that reveals the deep structural patterns underlying both type theory and proof theory. A **category** consists of objects and morphisms (arrows) between them, subject to composition and identity laws. The remarkable discovery of the 1960s and 1970s was that logical systems correspond to categories with specific structure, and that this correspondence illuminates both sides.

The simply typed lambda calculus, for instance, finds its natural home in **Cartesian closed categories** (CCCs). A CCC is a category with finite products (corresponding to conjunction / product types) and exponential objects (corresponding to implication / function types). The internal language of a CCC is precisely the simply typed lambda calculus: every CCC gives rise to a type theory, and every type theory gives rise to a CCC. This three-way correspondence — logic, type theory, category theory — is sometimes called the **Curry-Howard-Lambek correspondence**, after **Joachim Lambek**, who established the categorical leg of the triangle in the 1970s.

The most profound categorical contribution to logic is the theory of **elementary topoi**, introduced by **F. William Lawvere** and **Myles Tierney** around 1970. A topos is a category that behaves like a "universe of generalized sets." Formally, an elementary topos is a category with finite limits, exponentials, and a **subobject classifier** $\Omega$ — an object that plays the role of a "truth-value object." In the category $\mathbf{Set}$ of ordinary sets, $\Omega = \{0, 1\}$, and subsets of a set $X$ correspond to characteristic functions $X \to \Omega$. In a general topos, $\Omega$ may have more than two elements, and the internal logic of the topos need not be classical.

This is the key insight: **the internal logic of a topos is intuitionistic**. In every topos, one can define internal versions of conjunction, disjunction, implication, and quantification, and these satisfy the laws of intuitionistic logic. The law of excluded middle $P \lor \neg P$ may fail — not because of a philosophical commitment to constructivism, but because the structure of the topos does not support it. Classical logic emerges only when the subobject classifier satisfies the additional condition that $\Omega$ is a Boolean algebra, which holds in $\mathbf{Set}$ but not in general.

**Sheaf topoi** provide a particularly rich class of examples. Given a topological space $X$, the category $\mathbf{Sh}(X)$ of sheaves on $X$ is a topos whose internal logic reflects the topology of $X$. Open sets play the role of truth values, and a proposition is "true" on an open set $U$ if it holds locally on $U$. This construction connects directly to **forcing** in set theory: Cohen's forcing technique for proving independence results can be recast as working in the internal logic of a sheaf topos, and the generic filter corresponds to a generic point of the underlying topological space. The topos-theoretic perspective thus unifies the logical technique of forcing with the geometric idea of local truth.

The categorical semantics of dependent type theory goes further. **Categories with families** (Dybjer, 1996) and **comprehension categories** (Jacobs, 1993) provide the appropriate categorical frameworks for interpreting dependent types, $\Pi$-types, and $\Sigma$-types. In this setting, a context $\Gamma$ is an object, a type $A$ in context $\Gamma$ is a morphism into $\Gamma$, and a term $a : A$ is a section of that morphism. The entire apparatus of dependent type theory — substitution, weakening, context extension — finds precise categorical expression. The slogan captures it well: a topos is a universe of generalized sets with its own internal logic, and category theory is the language in which the deep structure of that logic is most clearly expressed.

## Homotopy Type Theory

The most revolutionary development in type theory since the Curry-Howard correspondence is **homotopy type theory** (HoTT), which emerged from the work of **Vladimir Voevodsky**, **Steve Awodey**, and **Michael Warren** beginning around 2006. The central insight is breathtaking in its simplicity: types are spaces, terms are points, and proofs of equality are paths.

In Martin-Lof type theory, every type $A$ comes equipped with an **identity type** $\text{Id}_A(a, b)$ for any two terms $a, b : A$, representing the proposition that $a$ and $b$ are equal. Classically, one might think of equality as a simple yes-or-no matter. But in homotopy type theory, a proof $p : \text{Id}_A(a, b)$ is interpreted as a **path** from $a$ to $b$ in the space $A$. Two proofs of the same equality — two paths between the same endpoints — may or may not be equal, and the proof of their equality is a **homotopy** (a path between paths). This structure continues upward: equalities between equalities between equalities, ad infinitum. Types thus carry the structure of **$\infty$-groupoids** — the fundamental objects of higher category theory and homotopy theory.

The cornerstone of HoTT is the **univalence axiom**, formulated by Voevodsky. In ordinary mathematics, we routinely treat isomorphic structures as interchangeable — we say "the cyclic group of order 3" as if there is only one, when in fact there are many isomorphic copies. Univalence makes this principle precise. Given a universe $\mathcal{U}$ and two types $A, B : \mathcal{U}$, the univalence axiom states:

$$(A \simeq B) \;\to\; (A =_{\mathcal{U}} B)$$

That is, if two types are equivalent (in the precise sense of having a function between them with both a left and a right inverse, up to homotopy), then they are identical as elements of the universe. Combined with the fact that identity gives rise to equivalence (transport along a path), this means equivalence and identity coincide. The univalence axiom has far-reaching consequences: it implies **function extensionality** (two functions are equal if they agree on all inputs), and it ensures that all constructions in type theory automatically respect equivalence — a property that in traditional foundations requires constant vigilance.

**Higher inductive types** (HITs) extend the language further by allowing types to be defined not only by their points (constructors) but also by their paths. The **circle** $S^1$ is defined as a higher inductive type with one point constructor $\text{base} : S^1$ and one path constructor $\text{loop} : \text{base} =_{S^1} \text{base}$. From this definition alone, one can develop the theory of fundamental groups, covering spaces, and basic homotopy theory — all synthetically, without reference to point-set topology or real analysis. Higher inductive types can also represent pushouts, suspensions, truncations, and quotients, providing a rich vocabulary for constructive homotopy theory.

The **HoTT Book** — *Homotopy Type Theory: Univalent Foundations of Mathematics* — was published in 2013 as the product of a remarkable collaborative effort during the IAS special year on univalent foundations. Written by dozens of contributors and composed entirely in a proof assistant, it demonstrated that substantial mathematics can be developed in this new framework. Synthetic homotopy theory in HoTT has yielded new proofs of classical results (such as the calculation of $\pi_1(S^1) \cong \mathbb{Z}$) and has opened up entirely new directions, including **cubical type theory** (Cohen, Coquand, Huber, Mortberg, 2015), which provides a computational interpretation of univalence, resolving the question of whether the univalence axiom is compatible with the constructive character of type theory. HoTT remains the most active frontier of type-theoretic logic today, with deep connections to higher category theory, algebraic topology, and the foundations of mathematics.

## Proof Assistants and Formalization

The practical realization of type theory in software has produced one of the most consequential developments in modern mathematics: **proof assistants** (also called interactive theorem provers) — programs that check whether a purported proof is correct, down to the last logical step. Every major proof assistant is built on a type theory, and the act of formalizing a theorem in one of these systems is the act of constructing a term of the appropriate type, in precise accordance with the Curry-Howard correspondence.

The major systems span a range of type-theoretic foundations. **Coq** (now also called **Rocq**), developed since 1984 at INRIA, is based on the **Calculus of Inductive Constructions** (CIC), a rich dependent type theory that combines the Calculus of Constructions (Coquand and Huet, 1988) with inductive types. Coq's tactic language allows users to construct proofs interactively, and its extraction mechanism can produce executable programs from constructive proofs. **Lean 4**, developed by Leonardo de Moura at Microsoft Research and now maintained by the Lean community, is built on a dependent type theory with a powerful elaboration engine and extensive automation. Its design emphasizes usability, and it has attracted a large community of mathematicians. **Agda**, developed primarily in Gothenburg, implements a version of Martin-Lof's intensional type theory with support for dependent pattern matching and a cubical mode for homotopy type theory. **Isabelle/HOL**, rooted in Church's simple type theory (higher-order logic), takes a different approach: it uses a relatively simple type system but compensates with powerful automation tools, including the Sledgehammer tactic that calls external automated theorem provers.

The formalization of substantial mathematics in these systems has accelerated dramatically. In 2005, **Georges Gonthier** and his team completed the formalization of the **four-color theorem** in Coq — a theorem whose original 1976 proof by Appel and Haken relied on extensive computer case-checking that many mathematicians found unsatisfying. The Coq formalization provided a machine-verified proof of the entire argument, including all the combinatorial case analysis, settling any lingering doubts. **Thomas Hales'** Flyspeck project, completed in 2014, formalized his proof of the **Kepler conjecture** (on optimal sphere packing) in HOL Light and Isabelle, a project that took over a decade and involved contributions from dozens of researchers.

Perhaps the most striking recent achievement is the **Liquid Tensor Experiment**. In December 2020, **Peter Scholze** — a Fields Medalist known for his work in arithmetic geometry — posted a challenge to the formalization community: verify a key technical result (Theorem 9.4 of his joint work with Dustin Clausen on condensed mathematics) in a proof assistant. The result was deep, novel, and at the frontier of current research. A team led by **Johan Commelin**, working in Lean, completed the formalization in mid-2022 — a landmark demonstration that proof assistants can handle mathematics at the cutting edge, not just classical results. Scholze himself reported that the formalization process revealed subtle points in the argument that had not been fully articulated in the original proof.

**Kevin Buzzard** of Imperial College London has articulated an ambitious vision: the eventual formalization of all of mathematics, or at least all of the mathematics that appears in a standard graduate curriculum. His Xena project has produced Lean formalizations of results in number theory, commutative algebra, and algebraic geometry, and he has argued that proof assistants will become standard tools in mathematical research within a generation. The future points toward **AI-assisted proof** — systems that combine the creativity of large language models with the rigor of type-theoretic verification. **Autoformalization**, the automatic translation of informal mathematical text into formal proof, is an active research area that could dramatically lower the barrier to using proof assistants. As type theory continues to develop, the boundary between human mathematics and machine mathematics grows ever more permeable, and the ancient dream of mechanizing reason — from Leibniz through Frege, Hilbert, and Turing — comes closer to realization with each passing year.
