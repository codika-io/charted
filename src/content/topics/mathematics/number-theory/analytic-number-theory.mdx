---
title: "Analytic Number Theory"
description: "The Riemann zeta function, the Prime Number Theorem, and L-functions."
parent: mathematics/number-theory
order: 5
color: "#ef4444"
difficulty: advanced
prerequisites: ["mathematics/number-theory/elementary-number-theory", "mathematics/analysis/complex-analysis"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Analytic number theory is the branch of mathematics that brings the tools of real and complex analysis to bear on questions about the integers — above all, on the distribution of prime numbers. The field was born in the mid-nineteenth century when Bernhard Riemann showed, in a single landmark 1859 paper, that the prime-counting function is controlled by the zeros of a complex-analytic function now called the Riemann zeta function, and it has grown into one of the richest and most active areas of modern mathematics, connecting elementary arithmetic to complex analysis, harmonic analysis, and representation theory. At its heart lies a stunning fact: the seemingly erratic sequence of prime numbers $2, 3, 5, 7, 11, \ldots$ is governed by a precise, beautiful law, and the tools needed to prove it come not from arithmetic at all, but from analysis.

## The Riemann Zeta Function

The **Riemann zeta function** is the central object of analytic number theory. For a complex number $s$ with real part greater than one, it is defined by the absolutely convergent Dirichlet series

$$\zeta(s) = \sum_{n=1}^{\infty} \frac{1}{n^s} = 1 + \frac{1}{2^s} + \frac{1}{3^s} + \frac{1}{4^s} + \cdots$$

The connection to prime numbers appears immediately. Leonhard Euler, working in the eighteenth century, observed that the fundamental theorem of arithmetic — unique factorization of integers into primes — translates directly into a product formula. Since every positive integer has a unique prime factorization, the sum over all integers splits into an independent product over all primes $p$:

$$\zeta(s) = \prod_{p \text{ prime}} \frac{1}{1 - p^{-s}}$$

This **Euler product** encodes all information about primes into the analytic function $\zeta(s)$. When $s = 1$, the sum becomes the harmonic series $1 + \frac{1}{2} + \frac{1}{3} + \cdots$, which diverges — and from this divergence Euler extracted a new proof that there are infinitely many primes, since if there were only finitely many, the product would converge to a finite value.

Riemann's 1859 paper *Ueber die Anzahl der Primzahlen unter einer gegebenen Grösse* (On the number of primes less than a given magnitude) transformed the subject by treating $\zeta(s)$ as a function of a complex variable. The series converges absolutely for $\text{Re}(s) > 1$, but Riemann showed that $\zeta(s)$ extends to a **meromorphic function** on the entire complex plane, with a single simple pole at $s = 1$ with residue $1$. The key tool is analytic continuation: starting from the Dirichlet series, one uses integral representations and functional equations to extend the function far beyond its original domain.

The central symmetry of $\zeta(s)$ is expressed by its **functional equation**, which Riemann established using the completed zeta function $\xi(s) = \frac{1}{2} s(s-1) \pi^{-s/2} \Gamma(s/2) \zeta(s)$. This completed function satisfies the elegant relation

$$\xi(s) = \xi(1 - s)$$

reflecting a deep symmetry about the line $\text{Re}(s) = \frac{1}{2}$. The gamma factor $\Gamma(s/2)$ introduces **trivial zeros** at the negative even integers $s = -2, -4, -6, \ldots$, where the gamma function has poles. All other zeros — the **non-trivial zeros** — lie inside the **critical strip** $0 < \text{Re}(s) < 1$. Riemann computed the first few non-trivial zeros and noticed they all lie on the **critical line** $\text{Re}(s) = \frac{1}{2}$. He conjectured, cautiously, that this is always the case.

The **Riemann Hypothesis** — that all non-trivial zeros of $\zeta(s)$ satisfy $\text{Re}(s) = \frac{1}{2}$ — remains unproven, and is widely regarded as the most important unsolved problem in mathematics. It is one of the Clay Millennium Prize Problems. Computational verification has confirmed the hypothesis for the first $10^{13}$ non-trivial zeros, all lying precisely on the critical line, but no proof is in sight. The stakes are high: the Riemann Hypothesis is equivalent to the sharpest possible error bound for the prime-counting function, and its truth (or falsity) would have sweeping consequences throughout number theory and beyond.

## Prime Number Theorem and its Proof

The **prime-counting function** $\pi(x)$ counts the number of primes less than or equal to $x$. The fundamental question — how fast does $\pi(x)$ grow? — occupied mathematicians for centuries. Empirical evidence accumulated by Gauss and Legendre in the late eighteenth century suggested an asymptotic law, and Gauss conjectured that $\pi(x)$ grows roughly like $x / \log x$ or like the **logarithmic integral**

$$\mathrm{Li}(x) = \int_2^x \frac{dt}{\log t}$$

The **Prime Number Theorem** makes this precise: as $x \to \infty$,

$$\pi(x) \sim \frac{x}{\log x}$$

which means $\pi(x) / (x / \log x) \to 1$. Equivalently, using the **Chebyshev function** $\psi(x) = \sum_{p^k \leq x} \log p$ (which weights prime powers by their logarithms), the theorem is equivalent to $\psi(x) \sim x$.

The Prime Number Theorem was proved independently in 1896 by **Jacques Hadamard** and **Charles-Jean de la Vallée Poussin**, using Riemann's approach. The key step is to show that $\zeta(s)$ has no zeros on the line $\text{Re}(s) = 1$ — the boundary of the critical strip. Specifically, one proves the estimate $\zeta(1 + it) \neq 0$ for all real $t \neq 0$. A clever trigonometric argument using the inequality $3 + 4\cos\theta + \cos 2\theta \geq 0$ establishes this zero-free region. Once this is known, contour integration via the **explicit formula** — which expresses $\psi(x)$ as a sum over zeros of $\zeta(s)$ — yields the asymptotic $\psi(x) \sim x$.

Riemann's explicit formula is the jewel at the center of this proof. It states, in a precise sense:

$$\psi(x) = x - \sum_{\rho} \frac{x^{\rho}}{\rho} - \log(2\pi) - \frac{1}{2} \log(1 - x^{-2})$$

where the sum runs over all non-trivial zeros $\rho$ of $\zeta(s)$. This formula shows that the zeros of $\zeta(s)$ act as "harmonics" in the distribution of primes: each zero contributes an oscillating term $x^{\rho}/\rho$, and the prime distribution is the superposition of all these oscillations. If all zeros satisfy $\text{Re}(\rho) = \frac{1}{2}$, as the Riemann Hypothesis asserts, then $|x^{\rho}| = \sqrt{x}$ for each term, and one obtains the conditional error bound

$$\psi(x) = x + O\!\left(\sqrt{x} \log^2 x\right)$$

or equivalently $\pi(x) = \mathrm{Li}(x) + O(\sqrt{x} \log x)$. Unconditionally, de la Vallée Poussin established a zero-free region of the form $\text{Re}(s) > 1 - c/\log |t|$ for an absolute constant $c > 0$, which yields the error term

$$\pi(x) = \mathrm{Li}(x) + O\!\left(x \, e^{-c\sqrt{\log x}}\right)$$

Improving this error term — and in particular reducing the exponent from $\sqrt{\log x}$ toward $x^{1/2}$ — is one of the central open problems in the field, and any progress is intimately tied to knowledge of the zero-free region for $\zeta(s)$.

## Dirichlet L-Functions and Primes in Progressions

The natural generalization asks not just how many primes there are up to $x$, but how many primes fall in a given **arithmetic progression** $a, a + q, a + 2q, \ldots$ for integers $a$ and $q$ with $\gcd(a, q) = 1$. The intuitive expectation is that primes are roughly equidistributed among the $\phi(q)$ residue classes modulo $q$ that are coprime to $q$.

This was made precise by **Peter Gustav Lejeune Dirichlet** in 1837 in his celebrated theorem: if $\gcd(a, q) = 1$, then there are infinitely many primes $p \equiv a \pmod{q}$. Dirichlet's proof introduced two fundamental innovations that shaped the next century of number theory.

The first innovation is **Dirichlet characters**: completely multiplicative functions $\chi : \mathbb{Z} \to \mathbb{C}$ of period $q$ satisfying $\chi(n) = 0$ when $\gcd(n, q) > 1$ and $|\chi(n)| = 1$ when $\gcd(n, q) = 1$. The characters modulo $q$ form a group of order $\phi(q)$ under pointwise multiplication, and they satisfy the crucial **orthogonality relations**:

$$\sum_{n=1}^{q} \chi(n) \overline{\chi'(n)} = \begin{cases} \phi(q) & \text{if } \chi = \chi' \\ 0 & \text{otherwise} \end{cases}$$

These orthogonality relations allow one to "detect" a residue class: the indicator of the condition $n \equiv a \pmod{q}$ is expressed as

$$\mathbf{1}[n \equiv a] = \frac{1}{\phi(q)} \sum_{\chi \bmod q} \overline{\chi(a)} \chi(n)$$

The second innovation is the **Dirichlet L-function** associated to a character $\chi$:

$$L(s, \chi) = \sum_{n=1}^{\infty} \frac{\chi(n)}{n^s} = \prod_{p} \frac{1}{1 - \chi(p) p^{-s}}$$

Like the zeta function, each $L(s, \chi)$ has an Euler product reflecting unique factorization, and extends to an entire function (if $\chi$ is non-principal) or a function with a single pole at $s = 1$ (for the principal character $\chi_0$, where $L(s, \chi_0)$ is essentially $\zeta(s)$). By combining the logarithmic derivative of $L(s, \chi)$ with the orthogonality of characters, Dirichlet reduced the count of primes in the progression $a \pmod{q}$ to a sum over all characters:

$$\sum_{\substack{p \leq x \\ p \equiv a \pmod{q}}} \log p \approx \frac{1}{\phi(q)} x$$

The key analytical step is proving that $L(1, \chi) \neq 0$ for all non-principal characters $\chi$. For complex characters this follows quickly from the Euler product, but for **real characters** (those with $\chi^2 = \chi_0$) Dirichlet needed a subtle argument involving class numbers of quadratic forms — a hint of the deep connections between L-functions and algebraic number theory.

The quantitative form, the **Siegel-Walfisz theorem**, states that for any $A > 0$ and for $q \leq (\log x)^A$, one has

$$\pi(x; q, a) = \frac{\mathrm{Li}(x)}{\phi(q)} + O\!\left(x \, e^{-c\sqrt{\log x}}\right)$$

where the constant $c$ depends only on $A$. The generalized Riemann hypothesis — that all zeros of all Dirichlet L-functions lie on the critical line — would sharpen this dramatically to an error of size $O(\sqrt{x} \log x)$.

The zeros of Dirichlet L-functions are controlled by the **zero-free regions** analogous to those for $\zeta(s)$. A notorious difficulty is the possible existence of a **Siegel zero**: a real zero of $L(s, \chi)$ very close to $s = 1$ for a real character $\chi$. The non-existence of Siegel zeros is essentially equivalent to improved bounds for primes in progressions, and despite enormous effort, this remains out of reach.

## Additive and Multiplicative Problems

Analytic number theory also attacks problems of an additive nature: when can an integer be expressed as a sum of primes, powers, or other special integers? These questions require a different toolkit — in particular, **exponential sums** and the **circle method**.

**Goldbach's conjecture**, stated in a 1742 letter from Christian Goldbach to Euler, asserts that every even integer greater than $2$ is the sum of two primes. It remains open, despite being verified computationally for all even numbers up to $4 \times 10^{18}$. The best proven result in this direction is **Chen's theorem** (1973), which establishes that every sufficiently large even integer is the sum of a prime and a number with at most two prime factors.

The **ternary Goldbach conjecture** — that every odd number greater than $5$ is the sum of three primes — was resolved in 2013 by **Harald Helfgott**, who proved it for all odd numbers greater than $5$ without exception. Earlier, **Ivan Vinogradov** had shown in 1937 that every sufficiently large odd number is the sum of three primes, using his method for bounding **Vinogradov exponential sums** of the form $\sum_{p \leq x} e^{2\pi i \alpha p}$.

The fundamental tool for these additive problems is the **Hardy-Littlewood circle method**, developed by **G. H. Hardy** and **John Edensor Littlewood** in the 1920s. The idea is to express the number of representations of $N$ as a sum of $k$ primes using an integral around the unit circle:

$$r_k(N) = \int_0^1 S(\alpha)^k e^{-2\pi i N \alpha} \, d\alpha$$

where $S(\alpha) = \sum_{p \leq N} e^{2\pi i \alpha p}$ is an exponential sum over primes. The circle is divided into **major arcs** (where $\alpha$ is close to a rational number with small denominator) and **minor arcs** (the rest). On major arcs, $S(\alpha)$ is well-approximated using knowledge of primes in arithmetic progressions, giving a main term called the **singular series** $\mathfrak{S}(N)$. On minor arcs, the exponential sum cancels due to equidistribution, giving a negligible error. The singular series is an explicit product over primes that equals $0$ for even $N$ (three-prime Goldbach) and is bounded away from zero for odd $N$, explaining why the conjecture should hold only for odd numbers.

**Waring's problem** asks for the minimum number $g(k)$ such that every positive integer is the sum of at most $g(k)$ perfect $k$-th powers. Lagrange's four-square theorem ($k = 2$, $g(2) = 4$) is the classical case. For general $k$, it was shown by **David Hilbert** in 1909 that $g(k)$ is always finite. The exact values are known for all $k$: $g(3) = 9$, $g(4) = 19$, and for large $k$ one has $g(k) = 2^k + \lfloor (3/2)^k \rfloor - 2$. The Hardy-Littlewood method again provides the main tool, though the minor arc estimates require substantial work.

On the **multiplicative** side, analytic number theory studies the average behavior of arithmetic functions. The **average order** of the divisor function $d(n)$ (which counts divisors) satisfies

$$\sum_{n \leq x} d(n) = x \log x + (2\gamma - 1)x + O(\sqrt{x})$$

where $\gamma$ is the Euler-Mascheroni constant. The error term $O(\sqrt{x})$ is classical (due to Dirichlet), but improving the exponent $\frac{1}{2}$ to the conjectured $\frac{1}{4} + \varepsilon$ is the **Dirichlet divisor problem**, still open. The **Erdős-Kac theorem** (1940) shows that the number of prime factors $\omega(n)$ of a "typical" integer near $x$ is normally distributed with mean and variance both equal to $\log \log x$ — a striking probabilistic statement about a completely deterministic function.

## Modular Forms and L-Functions

The deepest direction of modern analytic number theory leads into the theory of **modular forms** and their associated L-functions. A modular form is a complex-analytic function on the upper half-plane $\mathbb{H} = \{z \in \mathbb{C} : \text{Im}(z) > 0\}$ that transforms in a controlled way under the action of the modular group $\mathrm{SL}_2(\mathbb{Z})$. Specifically, a **modular form of weight $k$** for $\mathrm{SL}_2(\mathbb{Z})$ is a holomorphic function $f : \mathbb{H} \to \mathbb{C}$ satisfying

$$f\!\left(\frac{az + b}{cz + d}\right) = (cz + d)^k f(z) \quad \text{for all } \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in \mathrm{SL}_2(\mathbb{Z})$$

together with a growth condition at the cusp $i\infty$. If $f$ vanishes at the cusp, it is called a **cusp form**. Every modular form has a Fourier expansion $f(z) = \sum_{n=0}^{\infty} a_n q^n$ where $q = e^{2\pi i z}$, and the coefficients $a_n$ carry profound arithmetic information.

The **L-function** of a cusp form $f$ is defined by

$$L(s, f) = \sum_{n=1}^{\infty} \frac{a_n}{n^s}$$

These L-functions share all the key properties of the Riemann zeta function and Dirichlet L-functions: they have Euler products, analytic continuations, and functional equations. For an **eigenform** — a cusp form that is simultaneously an eigenfunction of all the **Hecke operators** $T_n$ — the Euler product takes the factored form

$$L(s, f) = \prod_{p} \frac{1}{1 - a_p p^{-s} + p^{k-1-2s}}$$

where $a_p$ is the Hecke eigenvalue at $p$. The multiplicativity of Hecke eigenvalues, established by Erich Hecke in the 1930s, implies the multiplicativity $a_{mn} = a_m a_n$ for $\gcd(m,n) = 1$, which is precisely what gives the Euler product.

The arithmetic significance of modular forms is best illustrated by the **Ramanujan tau function** $\tau(n)$, defined by the expansion of the cusp form $\Delta(z) = q \prod_{n=1}^{\infty} (1-q^n)^{24} = \sum_{n=1}^{\infty} \tau(n) q^n$. In 1916, Srinivasa Ramanujan conjectured that $|\tau(p)| \leq 2p^{11/2}$ for all primes $p$. This was proved in 1974 by **Pierre Deligne** as a consequence of his proof of the **Weil conjectures** — a profound connection between the theory of modular forms and algebraic geometry over finite fields, for which Deligne received the Fields Medal.

The most spectacular application of modular forms to number theory is the proof of **Fermat's Last Theorem**. In 1955, **Yutaka Taniyama** and **Goro Shimura** conjectured that every elliptic curve over $\mathbb{Q}$ is modular, meaning its L-function equals the L-function of some weight-2 cusp form. In 1986, **Ken Ribet** proved that the Taniyama-Shimura conjecture (for semistable curves) implies Fermat's Last Theorem. In 1995, **Andrew Wiles**, with crucial help from **Richard Taylor**, proved the semistable case of the conjecture, thereby establishing Fermat's Last Theorem after 358 years. The full modularity theorem — that every elliptic curve over $\mathbb{Q}$ is modular — was completed in 2001 by Breuil, Conrad, Diamond, and Taylor.

The general philosophy connecting arithmetic objects to automorphic L-functions is the **Langlands program**, initiated by **Robert Langlands** in a celebrated 1967 letter to André Weil. The program predicts a vast web of correspondences between Galois representations, automorphic forms, and L-functions. It subsumes class field theory, the modularity theorem, and Dirichlet's theorem as special cases, and its full realization would constitute a unification of number theory on a scale comparable to what the Langlands program itself calls "a kind of arithmetic Fourier analysis." Current research — on the geometric Langlands program, on the local and global correspondences for reductive groups, and on the applications to Diophantine equations — represents the frontier of analytic number theory and algebraic number theory combined, pointing toward a deep and still-unfolding synthesis of analysis, algebra, and arithmetic.
