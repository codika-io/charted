---
title: "Real Analysis"
description: "Sequences, continuity, differentiation, and Riemann/Lebesgue integration."
parent: mathematics/analysis
order: 1
color: "#ef4444"
difficulty: intermediate
prerequisites: ["mathematics/set-theory", "mathematics/logic"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Real analysis is the rigorous foundation of calculus — the discipline that supplies precise definitions and airtight proofs for the intuitions that Newton and Leibniz set in motion in the seventeenth century. It is where the notion of "approaching a limit" is given an exact meaning, where the familiar rules of differentiation and integration are derived from first principles, and where the real number line is examined with enough care to reveal its subtle completeness properties. Studying real analysis is, above all, an exercise in mathematical maturity: it trains the mind to distrust intuition just enough, and to trust proof absolutely.

## Foundations of the Real Number System

The story begins not with calculus but with numbers. The rational numbers $\mathbb{Q}$ — fractions $p/q$ with $p, q \in \mathbb{Z}$, $q \neq 0$ — seem, at first glance, to fill up the number line. The ancient Greeks believed precisely this until they proved, to their dismay, that $\sqrt{2}$ cannot be rational. The rationals have **gaps**, and those gaps are precisely the obstacle that prevents a naive theory of limits from working.

The **real numbers** $\mathbb{R}$ are constructed to fill those gaps. There are two classical constructions. **Richard Dedekind**, in his 1872 essay *Stetigkeit und irrationale Zahlen*, defined each real number as a **Dedekind cut**: a partition of $\mathbb{Q}$ into two non-empty sets $(A, B)$ such that every element of $A$ is less than every element of $B$ and $A$ has no largest element. The real number $\sqrt{2}$, for instance, corresponds to the cut where $A = \{q \in \mathbb{Q} : q \leq 0 \text{ or } q^2 < 2\}$. Simultaneously, **Georg Cantor** proposed defining real numbers as equivalence classes of **Cauchy sequences** of rationals — sequences that become arbitrarily close to each other without necessarily converging to a rational limit. Both constructions yield the same object: an **ordered field** with the crucial additional property called **completeness**.

Formally, $\mathbb{R}$ is characterized as the unique complete ordered field. It satisfies the usual field axioms (addition, multiplication, their inverses), an ordering compatible with the field structure, and the **Completeness Axiom** (also called the **Least Upper Bound Property**): every non-empty subset of $\mathbb{R}$ that is bounded above has a **supremum** (least upper bound) in $\mathbb{R}$. This single axiom is what distinguishes $\mathbb{R}$ from $\mathbb{Q}$. The supremum of a set $S$ is written $\sup S$; the greatest lower bound is the **infimum** $\inf S$.

Two important consequences flow immediately from completeness. The **Archimedean property** states that for every real number $x$, there exists a natural number $n$ with $n > x$ — the natural numbers are not bounded above in $\mathbb{R}$. This rules out infinitely large or infinitely small elements. The **density of the rationals** states that between any two distinct real numbers $a < b$ there exists a rational number $q$ with $a < q < b$. Remarkably, there also exists an irrational between $a$ and $b$, so both $\mathbb{Q}$ and $\mathbb{R} \setminus \mathbb{Q}$ are dense in $\mathbb{R}$, even though they have very different cardinalities (countable versus uncountable).

The **absolute value** $|x|$ measures distance from the origin: $|x| = x$ if $x \geq 0$ and $|x| = -x$ if $x < 0$. Its most important property is the **triangle inequality**:

$$|x + y| \leq |x| + |y| \quad \text{for all } x, y \in \mathbb{R}.$$

The triangle inequality is the engine that drives most convergence arguments. A variant, the reverse triangle inequality $\bigl||x| - |y|\bigr| \leq |x - y|$, is equally useful in bounding differences.

## Sequences and Series

A **sequence** of real numbers is a function $a : \mathbb{N} \to \mathbb{R}$, written $(a_n)_{n=1}^\infty$ or simply $(a_n)$. The central question is: does the sequence settle down to a definite value? A sequence $(a_n)$ **converges** to a limit $L \in \mathbb{R}$, written $a_n \to L$ or $\lim_{n \to \infty} a_n = L$, if for every $\varepsilon > 0$ there exists $N \in \mathbb{N}$ such that $|a_n - L| < \varepsilon$ for all $n > N$. In plain English: the terms of the sequence eventually stay within any prescribed distance $\varepsilon$ of $L$. This is the **$\varepsilon$-$N$ definition**, and mastering it — learning to produce an $N$ given an arbitrary $\varepsilon$ — is the first and most important technical skill of real analysis.

Limits, when they exist, are unique. Convergent sequences are necessarily **bounded**: there exists $M > 0$ with $|a_n| \leq M$ for all $n$. The algebra of limits is familiar from calculus but requires proof: if $a_n \to L$ and $b_n \to M$, then $a_n + b_n \to L + M$, $a_n b_n \to LM$, and $a_n / b_n \to L/M$ provided $M \neq 0$.

**Monotone sequences** are particularly well-behaved. The **Monotone Convergence Theorem** states that every monotone increasing sequence that is bounded above converges, and every monotone decreasing sequence that is bounded below converges. This is a direct consequence of the completeness axiom: the supremum of the sequence is its limit.

The **Bolzano-Weierstrass Theorem** (named for **Bernard Bolzano** and **Karl Weierstrass**, the latter of whom gave the modern rigorous treatment in the 1860s) asserts that every bounded sequence of real numbers has a convergent **subsequence**. A subsequence $(a_{n_k})_{k=1}^\infty$ is obtained by selecting an infinite increasing chain of indices $n_1 < n_2 < n_3 < \cdots$. Bolzano-Weierstrass is a compactness result in disguise, and it underpins the proofs of the Extreme Value Theorem and the Heine-Cantor Theorem.

A sequence $(a_n)$ is a **Cauchy sequence** if for every $\varepsilon > 0$ there exists $N$ such that $|a_m - a_n| < \varepsilon$ for all $m, n > N$. The terms become close to each other without reference to any proposed limit. The fundamental result is that, in $\mathbb{R}$, Cauchy sequences and convergent sequences are the same thing: a sequence converges if and only if it is Cauchy. This property, called **completeness** of $\mathbb{R}$, is what the Cantor construction is designed to guarantee.

An **infinite series** $\sum_{n=1}^\infty a_n$ is defined as the limit of the partial sums $S_N = \sum_{n=1}^N a_n$. The series converges if and only if $(S_N)$ converges as a sequence. A necessary condition for convergence is that $a_n \to 0$, but this condition is far from sufficient — the **harmonic series** $\sum 1/n$ diverges even though $1/n \to 0$, a fact first proved by **Nicole Oresme** in the fourteenth century. The standard convergence tests — comparison, ratio, root, and the alternating series test — each carve out sufficient conditions. A series $\sum a_n$ is **absolutely convergent** if $\sum |a_n|$ converges; absolute convergence implies convergence, and absolutely convergent series can be rearranged without changing their sum. **Conditionally convergent** series — those that converge but not absolutely — are far more delicate: by the **Riemann Rearrangement Theorem**, any conditionally convergent series can be rearranged to converge to any prescribed real number, or even to diverge to $\pm \infty$.

## Limits, Continuity, and Differentiation

For functions $f : \mathbb{R} \to \mathbb{R}$, the limit $\lim_{x \to c} f(x) = L$ means: for every $\varepsilon > 0$, there exists $\delta > 0$ such that $0 < |x - c| < \delta$ implies $|f(x) - L| < \varepsilon$. The condition $0 < |x - c|$ ensures we do not require anything about $f(c)$ itself. A function is **continuous at $c$** if $\lim_{x \to c} f(x) = f(c)$ — the limit exists, equals the function value, and the function is defined at $c$. Continuity has an equivalent sequential characterization: $f$ is continuous at $c$ if and only if $x_n \to c$ implies $f(x_n) \to f(c)$ for every sequence $(x_n)$.

Continuous functions on **closed bounded intervals** enjoy two celebrated properties. The **Extreme Value Theorem** (proved rigorously by Weierstrass) states that if $f : [a,b] \to \mathbb{R}$ is continuous, then $f$ attains its maximum and minimum values — there exist $c, d \in [a,b]$ with $f(c) \leq f(x) \leq f(d)$ for all $x \in [a,b]$. The **Intermediate Value Theorem** (traced to Bolzano's 1817 paper *Rein analytischer Beweis*) states that if $f : [a,b] \to \mathbb{R}$ is continuous and $f(a) < k < f(b)$, then there exists $c \in (a,b)$ with $f(c) = k$. The Intermediate Value Theorem is the rigorous underpinning of root-finding algorithms: every continuous function that changes sign must have a zero.

**Uniform continuity** is a stronger condition: $f$ is uniformly continuous on a set $S$ if for every $\varepsilon > 0$ there exists a single $\delta > 0$ (independent of the point) such that $|x - y| < \delta$ implies $|f(x) - f(y)| < \varepsilon$ for all $x, y \in S$. The **Heine-Cantor Theorem** asserts that every continuous function on a closed bounded interval is uniformly continuous — a powerful result with no analogue on open intervals ($f(x) = 1/x$ on $(0,1)$ is continuous but not uniformly so).

The **derivative** of $f$ at a point $c$ is defined as the limit of the difference quotient:

$$f'(c) = \lim_{h \to 0} \frac{f(c+h) - f(c)}{h},$$

provided this limit exists. Differentiability at $c$ implies continuity at $c$, but not conversely — continuity is strictly weaker. The standard differentiation rules (sum, product, quotient, chain) are theorems, not axioms. The most important theorems in differential calculus are the **mean value theorems**. **Rolle's Theorem** states that if $f$ is continuous on $[a,b]$, differentiable on $(a,b)$, and $f(a) = f(b)$, then there exists $c \in (a,b)$ with $f'(c) = 0$. The **Lagrange Mean Value Theorem** (the form most often called "the mean value theorem") generalizes this:

$$f'(c) = \frac{f(b) - f(a)}{b - a} \quad \text{for some } c \in (a,b).$$

This single result implies that a function with a positive derivative on an interval is increasing, that a function with a zero derivative is constant, and that differentiable functions cannot oscillate faster than their derivative allows. **Taylor's Theorem** extends the idea, approximating a sufficiently smooth function by a polynomial and providing an explicit formula for the remainder.

## Riemann Integration

The **Riemann integral**, formalized by **Bernhard Riemann** in his 1854 Habilitation thesis *Über die Darstellbarkeit einer Function durch eine trigonometrische Reihe*, gives a precise meaning to the area under a curve. The construction begins with **partitions**: a partition $P$ of $[a,b]$ is a finite collection of points $a = x_0 < x_1 < \cdots < x_n = b$. For each subinterval $[x_{i-1}, x_i]$, define the **upper sum** $U(f,P) = \sum_{i=1}^n M_i (x_i - x_{i-1})$ where $M_i = \sup_{x \in [x_{i-1},x_i]} f(x)$, and the **lower sum** $L(f,P) = \sum_{i=1}^n m_i (x_i - x_{i-1})$ where $m_i = \inf_{x \in [x_{i-1},x_i]} f(x)$.

A bounded function $f$ is **Riemann integrable** on $[a,b]$ if the infimum of all upper sums equals the supremum of all lower sums:

$$\inf_P U(f,P) = \sup_P L(f,P) = \int_a^b f(x)\, dx.$$

The **Riemann-Darboux criterion** for integrability states that $f$ is integrable if and only if for every $\varepsilon > 0$ there exists a partition $P$ with $U(f,P) - L(f,P) < \varepsilon$. From this criterion, two large classes of integrable functions emerge: all continuous functions on $[a,b]$ are integrable (Weierstrass), and all monotone functions on $[a,b]$ are integrable. More generally, functions with only finitely many discontinuities are integrable, as are functions whose set of discontinuities has measure zero.

The relationship between differentiation and integration is codified in the two **Fundamental Theorems of Calculus**. The first theorem states that if $f$ is continuous on $[a,b]$ and $F(x) = \int_a^x f(t)\, dt$, then $F$ is differentiable and $F'(x) = f(x)$ — integration produces an antiderivative. The second theorem states that if $f$ is integrable on $[a,b]$ and $G$ is any antiderivative of $f$, then $\int_a^b f(x)\, dx = G(b) - G(a)$. Together, these theorems establish that differentiation and integration are inverse operations, a fact that Newton and Leibniz used instinctively but that required two centuries of effort to prove with full rigor.

## Metric Spaces and Topology of Euclidean Space

Real analysis on the real line generalizes naturally to higher dimensions and to abstract spaces. A **metric space** is a pair $(X, d)$ where $X$ is a set and $d : X \times X \to [0, \infty)$ is a **metric** satisfying, for all $x, y, z \in X$: (i) $d(x,y) = 0$ if and only if $x = y$; (ii) $d(x,y) = d(y,x)$ (symmetry); (iii) $d(x,z) \leq d(x,y) + d(y,z)$ (triangle inequality). The Euclidean metric on $\mathbb{R}^n$ is $d(x,y) = \|x - y\| = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$. Other examples include the discrete metric, the supremum metric $d_\infty(f,g) = \sup|f-g|$ on spaces of functions, and the $p$-adic metric on number fields.

An **open ball** of radius $r$ centered at $x$ is the set $B(x,r) = \{y \in X : d(x,y) < r\}$. A set $U \subseteq X$ is **open** if every point of $U$ has an open ball entirely contained in $U$. A set $C$ is **closed** if its complement is open — equivalently, if it contains all its limit points. The open sets define the **topology** of the metric space, encoding the notion of nearness.

The critical topological notion for analysis is **compactness**. A subset $K$ of a metric space is **compact** if every open cover of $K$ has a finite subcover. In $\mathbb{R}^n$, the **Heine-Borel Theorem** gives a much simpler characterization: a subset of $\mathbb{R}^n$ is compact if and only if it is **closed and bounded**. Compact sets are the natural domain for the strongest theorems: continuous functions on compact sets are uniformly continuous (Heine-Cantor), attain their extreme values (Extreme Value Theorem), and their images are compact.

A subset $S$ of a metric space is **connected** if it cannot be written as a union of two disjoint non-empty open sets. In $\mathbb{R}$, the connected sets are precisely the intervals (including rays and the whole line). A stronger notion is **path connectedness**: $S$ is path connected if any two points in $S$ can be joined by a continuous path lying entirely in $S$. In $\mathbb{R}^n$, path connectedness implies connectedness.

A metric space is **complete** if every Cauchy sequence in it converges to a point in the space. Euclidean space $\mathbb{R}^n$ is complete; the rationals $\mathbb{Q}$ are not. Completeness is the precise property that prevents sequences from converging to "missing" points. The **Banach Fixed Point Theorem** (also called the Contraction Mapping Theorem) states that every contraction on a complete metric space has a unique fixed point, and that iterating the contraction from any starting point converges to it. This theorem is the backbone of many existence proofs in differential equations and numerical analysis.

## Sequences and Series of Functions

When functions, rather than numbers, form the terms of a sequence, a new subtlety emerges: there are two distinct notions of convergence. A sequence of functions $(f_n)$ on a set $S$ converges **pointwise** to $f$ if $f_n(x) \to f(x)$ for each fixed $x \in S$ — a separate limit condition for each point. It converges **uniformly** to $f$ if for every $\varepsilon > 0$ there exists $N$ (independent of $x$) such that $|f_n(x) - f(x)| < \varepsilon$ for all $n > N$ and all $x \in S$. Uniform convergence is a joint condition on the entire function simultaneously, while pointwise convergence permits the rate of convergence to vary arbitrarily from point to point.

The distinction matters enormously because pointwise limits can destroy properties that each individual function possesses. A pointwise limit of continuous functions need not be continuous: the sequence $f_n(x) = x^n$ on $[0,1]$ converges pointwise to the function that is $0$ on $[0,1)$ and $1$ at $x=1$, which is discontinuous. Under **uniform convergence**, however, the limit of a sequence of continuous functions is continuous, the limit can be integrated term-by-term, and (with an additional condition on the derivatives) the limit can be differentiated term-by-term. These exchange-of-limits results are among the most useful theorems in analysis, and their failure under mere pointwise convergence is one of the lessons that the nineteenth century had to learn painfully.

A **series of functions** $\sum_{n=1}^\infty f_n$ converges uniformly if the partial sums converge uniformly. The **Weierstrass M-test** gives a clean sufficient condition: if $|f_n(x)| \leq M_n$ for all $x$ and $\sum M_n < \infty$, then $\sum f_n$ converges uniformly and absolutely. **Power series** $\sum_{n=0}^\infty c_n (x-a)^n$ are the most important examples: each power series has a **radius of convergence** $R$ (given by the Cauchy-Hadamard formula $1/R = \limsup |c_n|^{1/n}$) such that the series converges absolutely and uniformly on compact subsets of $(a-R, a+R)$ and diverges for $|x-a| > R$. Within its interval of convergence, a power series can be differentiated and integrated term-by-term, and it defines a function whose Taylor coefficients are exactly $c_n = f^{(n)}(a)/n!$. This is the theory of **analytic functions** in the real setting, a precursor to complex analysis.

The **Weierstrass Approximation Theorem** (1885) states that every continuous function on a closed bounded interval can be approximated uniformly by polynomials. The theorem was a surprise: polynomials are special, yet they are dense in the space of all continuous functions. Karl Weierstrass proved the theorem constructively; later, **Sergei Bernstein** gave an elegant probabilistic proof in 1912 using the polynomials now bearing his name. The **Stone-Weierstrass Theorem** vastly generalizes the result, replacing polynomials by any subalgebra of continuous functions that separates points and contains constants.

## Introduction to Lebesgue Integration

The Riemann integral has a fundamental limitation: it struggles with functions that oscillate wildly or have too many discontinuities. The **Dirichlet function** — defined to be $1$ on the rationals and $0$ on the irrationals — is not Riemann integrable on any interval, even though it "should" have integral $0$ (the rationals are negligible). The Riemann integral is also poorly behaved with respect to limits: pointwise limits of Riemann integrable functions need not be Riemann integrable, and even when they are, one cannot always exchange the limit and the integral.

**Henri Lebesgue**, in his 1902 doctoral thesis *Intégrale, longueur, aire*, introduced a radically different approach. Instead of partitioning the domain (the $x$-axis) as Riemann did, Lebesgue partitioned the **range** (the $y$-axis) and measured the size of the set of $x$-values where the function takes values in each strip. This requires a theory of **measure**: a way to assign a "size" to subsets of $\mathbb{R}$ that generalizes the length of intervals.

A **$\sigma$-algebra** on a set $X$ is a collection $\mathcal{F}$ of subsets of $X$ that is closed under complements and countable unions (and hence countable intersections). A **measure** $\mu : \mathcal{F} \to [0, \infty]$ is a function satisfying $\mu(\emptyset) = 0$ and **countable additivity**: if $A_1, A_2, \ldots$ are pairwise disjoint sets in $\mathcal{F}$, then $\mu\bigl(\bigcup_{n=1}^\infty A_n\bigr) = \sum_{n=1}^\infty \mu(A_n)$. The **Lebesgue measure** $\lambda$ on $\mathbb{R}$ is the unique measure on the **Borel $\sigma$-algebra** that assigns $\lambda([a,b]) = b - a$ to every interval. Sets of Lebesgue measure zero — **null sets** — can be ignored for the purposes of integration. The Cantor set is a striking example: it is uncountable yet has Lebesgue measure zero.

A function $f : \mathbb{R} \to \mathbb{R}$ is **Lebesgue measurable** if the preimage $f^{-1}((a,\infty))$ is a measurable set for every $a \in \mathbb{R}$. The Lebesgue integral of a non-negative measurable function is built up in stages: first for **simple functions** (finite linear combinations of indicator functions of measurable sets), then for general non-negative functions as a supremum over simple functions bounded below them, and finally for general functions by splitting $f = f^+ - f^-$ into positive and negative parts.

The power of Lebesgue integration lies in its convergence theorems. The **Monotone Convergence Theorem** states that if $(f_n)$ is a sequence of non-negative measurable functions increasing pointwise to $f$, then $\int f_n \, d\lambda \to \int f \, d\lambda$. **Fatou's Lemma** gives a lower bound for lim inf. The crown jewel is the **Dominated Convergence Theorem**: if $f_n \to f$ pointwise almost everywhere and $|f_n| \leq g$ for an integrable dominating function $g$, then $\int f_n \, d\lambda \to \int f \, d\lambda$. This theorem is the rigorous tool that makes the exchange of limits and integrals legitimate, and it is indispensable in functional analysis, probability theory, and the theory of partial differential equations.

Every Riemann integrable function is Lebesgue integrable and the two integrals agree. The Lebesgue integral is strictly more general: the Dirichlet function is Lebesgue integrable with integral $0$. The precise characterization of Riemann integrability in Lebesgue's language is elegant: a bounded function is Riemann integrable if and only if its set of discontinuities has Lebesgue measure zero. Real analysis, which began with the effort to make calculus rigorous, ends by opening the door to measure theory, functional analysis, and modern probability — the landscape of twentieth-century mathematics.
