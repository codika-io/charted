---
title: "Machine Learning"
description: "The study of algorithms that learn from data — supervised, unsupervised, and reinforcement learning, with statistical foundations."
parent: computer-science/ai-and-machine-learning
order: 1
color: "#3b82f6"
difficulty: intermediate
prerequisites: ["computer-science/theoretical-foundations/algorithms-and-complexity"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Machine learning is the study of algorithms that improve their performance at some task through experience, without being explicitly programmed for every contingency. It is the engine behind modern AI and the mathematical discipline that transforms raw data into predictions, decisions, and representations. The field draws on statistics, optimization, and linear algebra, and its results now underpin applications from medical diagnosis and language translation to autonomous driving and scientific discovery.

## Foundations and Learning Paradigms

The central question of machine learning is deceptively simple: given a finite sample of data, how can a system generalize to unseen examples? **Arthur Samuel** coined the term "machine learning" in 1959 while building a checkers-playing program at IBM, but the theoretical foundations were laid over the following decades by statisticians and computer scientists who formalized what it means for a learner to succeed.

The field is organized around three major **learning paradigms**. In **supervised learning**, the system receives labeled examples — pairs $(x_i, y_i)$ where $x_i$ is an input and $y_i$ is the correct output — and must learn a function $f$ such that $f(x) \approx y$ for new, unseen inputs. Classification (where $y$ is a discrete label) and regression (where $y$ is a continuous value) are the two main variants. In **unsupervised learning**, the system receives only inputs $x_1, x_2, \ldots, x_n$ with no labels, and must discover structure — clusters, low-dimensional representations, or density models — hidden in the data. In **reinforcement learning**, an agent interacts with an environment, receives rewards or penalties for its actions, and must learn a policy that maximizes cumulative reward over time.

Two concepts sit at the heart of every learning problem. **Generalization** is the ability to perform well on data the learner has never seen before; a model that memorizes its training set but fails on new examples has **overfit**. The **bias-variance tradeoff** captures this tension mathematically. A model's expected prediction error decomposes as the sum of its squared bias (error from incorrect assumptions in the model), its variance (sensitivity to fluctuations in the training set), and irreducible noise. Simple models tend to have high bias but low variance; complex models have low bias but high variance. The art of machine learning lies in finding the sweet spot.

The **No-Free-Lunch theorem**, proved by David Wolpert and William Macready in 1997, states that no single learning algorithm dominates all others across every possible problem. Averaged over all possible data distributions, every algorithm performs equally well. This result underscores the importance of matching algorithms to the structure of real-world problems rather than searching for a universal method.

## Statistical Learning Theory

The theoretical backbone of machine learning is **statistical learning theory**, developed primarily by **Vladimir Vapnik** and **Alexey Chervonenkis** beginning in the 1960s. Their central contribution was to formalize the conditions under which a learning algorithm can be guaranteed to generalize.

The **Probably Approximately Correct (PAC)** framework, introduced by **Leslie Valiant** in 1984, makes these guarantees precise. A concept class $\mathcal{C}$ is PAC-learnable if there exists an algorithm that, for any target concept $c \in \mathcal{C}$ and any distribution over inputs, will with probability at least $1 - \delta$ produce a hypothesis whose error is at most $\epsilon$, using a number of samples polynomial in $1/\epsilon$, $1/\delta$, and the complexity of $\mathcal{C}$. The framework separates what is learnable in principle from what is not, and it gives explicit **sample complexity** bounds — how many training examples are needed for a given level of confidence.

The key measure of hypothesis-class complexity in this theory is the **Vapnik-Chervonenkis (VC) dimension**. The VC dimension of a class $\mathcal{H}$ of binary classifiers is the size of the largest set of points that $\mathcal{H}$ can **shatter** — that is, classify in all $2^n$ possible ways. For example, the class of linear classifiers in $\mathbb{R}^2$ has VC dimension 3: any three non-collinear points can be shattered, but no set of four points can. The VC dimension provides distribution-free generalization bounds: with high probability, the true error of a learned hypothesis is at most its empirical error plus a term proportional to $\sqrt{d_{\text{VC}} / n}$, where $n$ is the sample size. These bounds, while often loose in practice, are among the deepest results in the theory.

**Rademacher complexity** offers a more refined, data-dependent measure of complexity that can yield tighter bounds for specific problems. Where VC dimension is a combinatorial quantity attached to the hypothesis class alone, Rademacher complexity measures how well the class can fit random noise in the actual data distribution, making it sensitive to the particular problem at hand.

## Linear Models and Optimization

The simplest and most thoroughly understood family of machine learning models is the **linear model**. In **linear regression**, the hypothesis takes the form $f(x) = w^T x + b$, where $w$ is a weight vector and $b$ is a bias term. The standard objective is to minimize the **mean squared error** over the training set:

$$\mathcal{L}(w) = \frac{1}{n} \sum_{i=1}^{n} (y_i - w^T x_i - b)^2$$

When the data matrix has full rank, the closed-form solution is given by the **normal equations**: $w^* = (X^T X)^{-1} X^T y$. For large-scale problems, however, iterative methods are preferred. **Gradient descent** updates the weights by stepping in the direction opposite to the gradient: $w \leftarrow w - \eta \nabla \mathcal{L}(w)$, where $\eta$ is the **learning rate**. **Stochastic gradient descent (SGD)**, introduced in various forms by Herbert Robbins and Sutton Monro in 1951, approximates the full gradient using a single example or a small mini-batch, trading accuracy per step for dramatic gains in speed. Modern variants — **Adam** (Diederik Kingma and Jimmy Ba, 2014), **RMSprop**, **AdaGrad** — adapt the learning rate per parameter, accelerating convergence on ill-conditioned landscapes.

**Regularization** prevents overfitting by penalizing model complexity. **L2 regularization** (Ridge regression) adds a term $\lambda \|w\|_2^2$ to the loss, shrinking all weights toward zero. **L1 regularization** (Lasso), proposed by Robert Tibshirani in 1996, adds $\lambda \|w\|_1$, which encourages **sparsity** — driving many weights exactly to zero and performing implicit feature selection. **Elastic Net** combines both penalties, inheriting the strengths of each.

**Logistic regression** extends the linear framework to binary classification by passing the linear combination through the **sigmoid function** $\sigma(z) = 1/(1 + e^{-z})$, which maps any real number to a probability in $(0, 1)$. The model is trained by minimizing the **cross-entropy loss**, and it generalizes to multi-class problems via the **softmax function**, which transforms a vector of scores into a probability distribution. Despite its name, logistic regression is a classification method, not a regression method, and it remains one of the most widely used models in practice because of its simplicity, interpretability, and strong performance on linearly separable problems.

## Support Vector Machines and Kernel Methods

**Support vector machines (SVMs)**, developed by Vapnik and his collaborators in the 1990s, formalize the intuition that the best classifier is the one that separates classes with the widest possible margin. Given labeled training data $\{(x_i, y_i)\}$ with $y_i \in \{-1, +1\}$, the **hard-margin SVM** seeks the hyperplane $w^T x + b = 0$ that correctly classifies all examples and maximizes the geometric margin $2/\|w\|$. This is equivalent to minimizing $\|w\|^2$ subject to $y_i(w^T x_i + b) \geq 1$ for all $i$ — a convex quadratic program.

When data is not perfectly separable, the **soft-margin** formulation introduces **slack variables** $\xi_i \geq 0$ that allow misclassifications at a cost controlled by a parameter $C$. The objective becomes minimizing $\frac{1}{2}\|w\|^2 + C \sum_i \xi_i$. A remarkable property of SVMs is that the solution depends only on the **support vectors** — the training points that lie on or violate the margin boundary — making the model sparse and interpretable.

The true power of SVMs emerges through the **kernel trick**. The dual formulation of the SVM optimization problem depends on the data only through inner products $x_i^T x_j$. By replacing these with a **kernel function** $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$, where $\phi$ maps inputs into a high-dimensional (possibly infinite-dimensional) feature space, the SVM can learn nonlinear decision boundaries without ever computing the transformation $\phi$ explicitly. The **radial basis function (RBF) kernel** $K(x, x') = \exp(-\gamma \|x - x'\|^2)$ maps data into an infinite-dimensional space and is the most popular choice. **Mercer's theorem** provides the mathematical foundation, characterizing which functions $K$ correspond to valid inner products in some feature space.

## Decision Trees and Ensemble Methods

**Decision trees** partition the input space into axis-aligned regions by recursively splitting on individual features. At each internal node, the algorithm chooses the feature and threshold that best separates the classes according to a splitting criterion — typically **information gain** (used in Ross Quinlan's **ID3** and **C4.5** algorithms, developed in the 1980s and 1990s) or the **Gini impurity** (used in Leo Breiman's **CART** algorithm). Trees are intuitive, interpretable, and handle both numerical and categorical features, but they are prone to overfitting. **Pruning** — removing branches that provide little predictive benefit — mitigates this, but a single tree often has high variance.

**Ensemble methods** address the variance problem by combining multiple models. **Bagging** (bootstrap aggregating), proposed by Breiman in 1996, trains many trees on bootstrap samples of the training data and averages their predictions, reducing variance without increasing bias. **Random forests**, also due to Breiman (2001), extend bagging by additionally randomizing the set of features considered at each split, decorrelating the trees and further reducing variance. The **out-of-bag error** provides a built-in estimate of generalization performance without a separate validation set.

**Boosting** takes a different approach, building an ensemble sequentially so that each new model focuses on the mistakes of its predecessors. **AdaBoost**, introduced by Yoav Freund and Robert Schapire in 1997, assigns increasing weights to misclassified examples and combines weak learners through a weighted vote. **Gradient boosting**, generalized by Jerome Friedman in 2001, frames boosting as gradient descent in function space, fitting each new model to the residuals of the ensemble so far. Modern implementations — **XGBoost**, **LightGBM**, **CatBoost** — add regularization, efficient tree-building heuristics, and parallelism, and they dominate structured-data competitions to this day. Feature importance scores derived from boosted ensembles are among the most widely used tools in applied data science.

## Neural Networks and Deep Learning

The idea of learning with artificial neurons dates to **Warren McCulloch** and **Walter Pitts**, who proposed a mathematical model of neural computation in 1943, and to **Frank Rosenblatt**, whose **perceptron** algorithm (1958) was the first trainable neural classifier. The perceptron learns a linear decision boundary by iteratively adjusting weights when it makes errors. Minsky and Papert's 1969 book *Perceptrons* demonstrated the model's inability to learn non-linearly separable functions like XOR, contributing to a long period of reduced interest.

The revival came with the **backpropagation algorithm**, popularized by David Rumelhart, Geoffrey Hinton, and Ronald Williams in 1986 (though discovered independently by several researchers earlier). Backpropagation computes the gradient of a loss function with respect to every weight in a **multi-layer perceptron (MLP)** by applying the chain rule of calculus layer by layer, from the output back to the input. Combined with nonlinear **activation functions** — sigmoid, tanh, and later the **rectified linear unit (ReLU)** — multi-layer networks can approximate any continuous function, a result formalized in the **universal approximation theorem** by George Cybenko (1989) and Kurt Hornik (1991).

**Deep learning** refers to the training of neural networks with many layers, and it became practical through a combination of algorithmic advances and hardware (GPU) acceleration in the 2010s. **Convolutional neural networks (CNNs)**, pioneered by Yann LeCun in the late 1980s for handwritten digit recognition, exploit the spatial structure of images through local receptive fields, weight sharing, and pooling. The **AlexNet** architecture (Alex Krizhevsky, Ilya Sutskever, and Hinton, 2012) won the ImageNet competition by a large margin and ignited the deep learning revolution. Subsequent architectures — **VGG**, **GoogLeNet**, **ResNet** with its skip connections (Kaiming He et al., 2015) — pushed performance ever higher while revealing that deeper networks, when properly designed, learn richer hierarchical representations.

**Recurrent neural networks (RNNs)** process sequential data by maintaining a hidden state that is updated at each time step. The vanilla RNN suffers from the **vanishing gradient problem** — gradients shrink exponentially as they are propagated through many time steps, making it difficult to learn long-range dependencies. The **Long Short-Term Memory (LSTM)** cell, introduced by Sepp Hochreiter and Jurgen Schmidhuber in 1997, solves this with gating mechanisms that control the flow of information. The **Gated Recurrent Unit (GRU)**, proposed by Kyunghyun Cho and colleagues in 2014, simplifies the LSTM design while retaining much of its effectiveness.

## Transformers and Generative Models

The **transformer** architecture, introduced by Ashish Vaswani and colleagues in the 2017 paper *Attention Is All You Need*, replaced recurrence with **self-attention** — a mechanism that allows every position in a sequence to attend to every other position in parallel. The core operation computes attention weights as scaled dot products between learned query, key, and value representations:

$$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

**Multi-head attention** runs several attention functions in parallel with different learned projections, allowing the model to capture different types of relationships simultaneously. **Positional encodings** (sinusoidal or learned) inject sequence-order information that the attention mechanism itself does not inherently encode. The transformer's parallelizability over sequence length made it far more efficient to train than RNNs on modern hardware, and it quickly became the dominant architecture across natural language processing, computer vision, and beyond.

The transformer spawned two families of **pre-trained language models**. **BERT** (Jacob Devlin et al., 2018) uses a bidirectional encoder trained with masked language modeling — randomly masking tokens and predicting them from context — yielding rich contextual representations that can be fine-tuned for downstream tasks. The **GPT** series (OpenAI, 2018 onward) uses a unidirectional (autoregressive) decoder trained to predict the next token, scaling to billions and then hundreds of billions of parameters. **Scaling laws**, identified by Jared Kaplan and colleagues in 2020, revealed smooth power-law relationships between model size, data size, compute, and loss, guiding the construction of ever-larger models and predicting **emergent abilities** — capabilities that appear abruptly as models cross certain scale thresholds.

**Generative models** more broadly aim to learn the distribution of the training data so that new samples can be synthesized. **Variational autoencoders (VAEs)**, introduced by Diederik Kingma and Max Welling in 2013, learn a latent representation by jointly training an encoder and a decoder while regularizing the latent space to follow a known distribution. **Generative adversarial networks (GANs)**, proposed by Ian Goodfellow and colleagues in 2014, pit a generator against a discriminator in a minimax game, producing startlingly realistic images but suffering from training instability and **mode collapse**. **Diffusion models**, building on work by Jascha Sohl-Dickstein and colleagues (2015) and substantially advanced by Jonathan Ho and colleagues (2020), learn to reverse a gradual noising process, achieving state-of-the-art image and audio generation with more stable training than GANs.

## Reinforcement Learning

**Reinforcement learning (RL)** formalizes the problem of sequential decision-making under uncertainty. The standard framework is the **Markov decision process (MDP)**, defined by a set of states $\mathcal{S}$, a set of actions $\mathcal{A}$, transition probabilities $P(s' | s, a)$, a reward function $R(s, a)$, and a discount factor $\gamma \in [0, 1)$. The agent's goal is to find a **policy** $\pi(a | s)$ that maximizes the expected cumulative discounted reward $\mathbb{E}\!\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right]$.

The **Bellman equations** relate the value of a state to the values of successor states. For the **state-value function** $V^\pi(s) = \mathbb{E}_\pi\!\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s\right]$, the Bellman equation is $V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a) + \gamma V^\pi(s')]$. **Dynamic programming** methods — **value iteration** and **policy iteration** — solve these equations exactly when the MDP model is known, but in most practical settings the model is unknown and the agent must learn from interaction.

**Model-free** methods learn directly from experience. **Q-learning**, introduced by Christopher Watkins in 1989, maintains an estimate of the action-value function $Q(s, a)$ and updates it toward the bootstrapped target $R + \gamma \max_{a'} Q(s', a')$ after each transition. Q-learning is **off-policy** — it learns about the optimal policy regardless of the policy used to collect data. **SARSA** is its on-policy counterpart. **Temporal difference (TD) learning** generalizes both, updating value estimates based on the difference between successive predictions.

**Deep Q-Networks (DQN)**, introduced by DeepMind in 2015, extended Q-learning to high-dimensional state spaces by approximating $Q$ with a deep neural network, achieving superhuman performance on Atari games. **Policy gradient** methods, notably the **REINFORCE** algorithm, optimize the policy directly by estimating the gradient of expected reward with respect to policy parameters. **Actor-critic** methods combine policy gradients with value-function baselines to reduce variance, and practical algorithms like **Proximal Policy Optimization (PPO)** and **Trust Region Policy Optimization (TRPO)** add constraints that stabilize training. The most celebrated RL achievement remains **AlphaGo** (DeepMind, 2016), which combined deep neural networks, Monte Carlo tree search, and self-play reinforcement learning to defeat the world champion at Go — a game long considered beyond the reach of AI.

## Bayesian Methods and Modern Frontiers

**Bayesian machine learning** treats model parameters as random variables with prior distributions and updates beliefs in light of data via **Bayes' theorem**: $P(\theta | D) \propto P(D | \theta) P(\theta)$. The **posterior distribution** $P(\theta | D)$ captures uncertainty about the parameters and enables principled predictions through marginalization. **Maximum a posteriori (MAP)** estimation finds the single most probable parameter setting and corresponds to regularized maximum likelihood — L2 regularization, for instance, arises from a Gaussian prior. Full Bayesian inference, by contrast, averages over all parameter settings, weighted by their posterior probability.

**Probabilistic graphical models** represent complex joint distributions using graph structures. **Bayesian networks** encode conditional independence assumptions in directed acyclic graphs, where each variable depends only on its parents. **Markov random fields** use undirected graphs to represent symmetric dependencies. Exact inference in these models — computing marginal or conditional probabilities — is often intractable, but algorithms like **belief propagation**, **variational inference**, and **Markov chain Monte Carlo (MCMC)** sampling provide powerful approximations. **Gaussian processes** offer a non-parametric Bayesian approach to regression, placing a prior directly over functions and providing uncertainty estimates alongside predictions.

The modern frontiers of machine learning extend in many directions. **Foundation models** — large models pre-trained on broad data and adapted to specific tasks — have become the dominant paradigm, raising questions about **scaling laws**, **emergent abilities**, and the boundary between memorization and understanding. **Self-supervised learning** methods like contrastive learning (SimCLR, MoCo) and masked modeling learn rich representations without labels. **Meta-learning** ("learning to learn") trains models that can adapt to new tasks from very few examples. **Federated learning** enables training across decentralized data sources while preserving privacy. And the field of **AI safety and alignment** grapples with ensuring that increasingly powerful learning systems remain beneficial and aligned with human values — a challenge that grows more urgent with every advance in scale and capability.
