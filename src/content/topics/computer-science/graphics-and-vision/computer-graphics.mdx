---
title: "Computer Graphics"
description: "The generation of visual content by computer — rendering, modeling, animation, and real-time graphics pipelines."
parent: computer-science/graphics-and-vision
order: 1
color: "#3b82f6"
difficulty: intermediate
prerequisites: ["computer-science/theoretical-foundations/algorithms-and-complexity"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Computer graphics is the discipline of creating visual content through computation — transforming mathematical descriptions of geometry, light, and motion into images that can be displayed on a screen, printed, or projected into virtual environments. It is the engine behind film visual effects, video games, scientific visualization, architectural design, and the graphical user interfaces through which billions of people interact with computers every day. The field rests on a foundation of linear algebra, calculus, and physics, binding abstract mathematics to the very concrete goal of producing pictures.

## The Graphics Pipeline and Coordinate Transformations

The central organizational concept in real-time computer graphics is the **graphics pipeline**, a sequence of processing stages that transforms a mathematical description of a three-dimensional scene into a two-dimensional image on screen. The pipeline concept emerged from hardware and software architectures developed at Silicon Graphics in the 1980s and was codified in APIs like OpenGL, introduced in 1992, and later DirectX and Vulkan. Understanding the pipeline means understanding how geometric data flows from application code through vertex processing, primitive assembly, rasterization, fragment processing, and output merging.

The journey begins with **coordinate transformations**. Objects in a scene are typically authored in their own local coordinate systems, called **model space** or **object space**. A model transformation places each object into a shared **world space**. A view transformation then re-expresses everything relative to a virtual camera, producing **camera space** or **eye space**. Finally, a projection transformation maps the three-dimensional camera space into a two-dimensional **clip space**, after which perspective division and viewport mapping produce the final pixel coordinates. Each of these transformations is represented as a matrix, and their composition into a single matrix is one of the great practical advantages of the approach.

The mathematical language of these transformations is **homogeneous coordinates**, in which a point $(x, y, z)$ in three-dimensional space is represented as the four-component vector $(x, y, z, 1)$. This seemingly redundant representation allows translation — which is not a linear operation on three-component vectors — to be expressed as matrix multiplication, unifying translation, rotation, scaling, and projection into a single framework of $4 \times 4$ matrices. An **affine transformation** is any composition of linear transformations and translations; in homogeneous coordinates, affine transformations are exactly the $4 \times 4$ matrices whose bottom row is $(0, 0, 0, 1)$. Projective transformations generalize further, allowing the bottom row to vary and enabling perspective projection, where distant objects appear smaller. The composition of model, view, and projection matrices is often written as the **MVP matrix**, and applying it to every vertex in a mesh is the first computational step in the graphics pipeline.

The idea of separating the pipeline into discrete stages originated in hardware design — each stage can operate in parallel on different primitives, achieving throughput measured in billions of triangles per second on modern GPUs. **Ivan Sutherland** and his students at the University of Utah, particularly **Edwin Catmull** and **John Warnock**, were instrumental in developing the foundational algorithms of the pipeline during the late 1960s and 1970s. Catmull's invention of the z-buffer (depth buffer) algorithm in 1974 solved the **hidden surface problem** — determining which surfaces are visible from the camera — in a way that was both simple and amenable to hardware acceleration, and it remains the standard approach fifty years later.

## Rasterization and the Fragment Pipeline

Once vertices have been transformed into screen coordinates, the pipeline must determine which pixels each geometric primitive covers and compute a color for each covered pixel. This process is called **rasterization**, and for triangles — the dominant primitive in modern graphics — it is both elegant and efficient.

**Triangle rasterization** works by testing each pixel in the triangle's bounding box to determine whether the pixel center lies inside the triangle. The standard approach uses **edge functions**: for a triangle with vertices $\mathbf{v}_0$, $\mathbf{v}_1$, $\mathbf{v}_2$, the edge function for the edge from $\mathbf{v}_0$ to $\mathbf{v}_1$ evaluated at a point $\mathbf{p}$ is the signed area of the parallelogram formed by the edge vector and the vector from $\mathbf{v}_0$ to $\mathbf{p}$. A point lies inside the triangle if and only if all three edge functions have the same sign. This test is both simple and parallelizable, making it ideal for GPU implementation.

Once a pixel is determined to lie inside a triangle, its attributes — position, normal, texture coordinates, color — are interpolated from the triangle's vertices using **barycentric coordinates**. Given a point $\mathbf{p}$ inside triangle $\mathbf{v}_0 \mathbf{v}_1 \mathbf{v}_2$, its barycentric coordinates $(\lambda_0, \lambda_1, \lambda_2)$ satisfy $\mathbf{p} = \lambda_0 \mathbf{v}_0 + \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2$ with $\lambda_0 + \lambda_1 + \lambda_2 = 1$. Any vertex attribute $a$ is then interpolated as $a(\mathbf{p}) = \lambda_0 a_0 + \lambda_1 a_1 + \lambda_2 a_2$. When perspective projection is involved, a correction called **perspective-correct interpolation** is necessary to avoid visual artifacts: attributes must be divided by the homogeneous $w$-coordinate before interpolation and then multiplied back afterward.

The interpolated attributes are passed to the **fragment shader** (also called a pixel shader), a programmable stage where per-pixel computations — lighting, texturing, shadowing — are performed. Fragment shaders are among the most powerful and creative tools in real-time graphics: they execute a small program for every fragment (candidate pixel) the rasterizer produces, and they can sample textures, compute lighting equations, apply procedural effects, and discard fragments entirely. After the fragment shader, the pipeline performs **depth testing** (comparing the fragment's depth against the z-buffer to determine visibility), **stencil testing** (for masking and special effects), and **blending** (combining the fragment's color with the existing framebuffer contents, essential for transparency). The final result is written to the **framebuffer**, and the process of **double buffering** — rendering to an off-screen buffer while displaying the previous frame — ensures smooth, flicker-free animation.

## Shading, Lighting, and Materials

The visual richness of a rendered scene comes largely from how surfaces interact with light. **Shading models** are mathematical descriptions of this interaction, ranging from simple empirical formulas to physically accurate simulations of light transport.

The foundational model is the **Phong illumination model**, introduced by **Bui Tuong Phong** in 1975. It decomposes the reflected light at a surface point into three components: **ambient** light, which approximates the overall diffuse illumination of the scene; **diffuse** reflection, proportional to the cosine of the angle between the surface normal $\mathbf{n}$ and the light direction $\mathbf{l}$, following **Lambert's cosine law** ($I_d = k_d (\mathbf{n} \cdot \mathbf{l})$); and **specular** reflection, which produces bright highlights and depends on the angle between the reflected light direction and the viewer direction, raised to a **shininess exponent** $\alpha$ that controls the tightness of the highlight. The complete Phong model is:

$$I = k_a I_a + k_d (\mathbf{n} \cdot \mathbf{l}) I_l + k_s (\mathbf{r} \cdot \mathbf{v})^\alpha I_l$$

where $\mathbf{r}$ is the reflection of $\mathbf{l}$ about $\mathbf{n}$, $\mathbf{v}$ is the view direction, and $k_a$, $k_d$, $k_s$ are material-dependent coefficients. **Jim Blinn** proposed a modification in 1977 — the **Blinn-Phong model** — that replaces the reflection vector with the **halfway vector** $\mathbf{h} = \frac{\mathbf{l} + \mathbf{v}}{|\mathbf{l} + \mathbf{v}|}$, computing the specular term as $k_s (\mathbf{n} \cdot \mathbf{h})^\alpha$. This is both more efficient and more physically plausible for certain geometries.

Modern rendering has moved toward **physically-based rendering (PBR)**, which models materials using the **Cook-Torrance** microfacet model. PBR describes a surface as a collection of tiny mirrors (microfacets) oriented according to a statistical distribution. The model uses a **normal distribution function** $D(\mathbf{h})$ describing the concentration of microfacets aligned with the halfway vector, a **Fresnel term** $F(\mathbf{v}, \mathbf{h})$ accounting for the angle-dependent reflectivity of dielectric and metallic surfaces, and a **geometry term** $G(\mathbf{l}, \mathbf{v})$ modeling self-shadowing among microfacets. The resulting bidirectional reflectance distribution function (BRDF) is:

$$f_r(\mathbf{l}, \mathbf{v}) = \frac{D(\mathbf{h}) \, F(\mathbf{v}, \mathbf{h}) \, G(\mathbf{l}, \mathbf{v})}{4 (\mathbf{n} \cdot \mathbf{l})(\mathbf{n} \cdot \mathbf{v})}$$

PBR materials are typically parameterized by a **base color** (albedo), **metalness** (whether the surface is a metal or dielectric), and **roughness** (controlling the spread of the microfacet distribution). This approach, which obeys the principle of **energy conservation** — a surface cannot reflect more light than it receives — produces consistent, realistic appearance under any lighting condition and has become the industry standard in games, film, and product visualization since its widespread adoption around 2013.

## Texture Mapping and Surface Detail

Real-world surfaces exhibit complex patterns — wood grain, fabric weave, rust, skin pores — that would be prohibitively expensive to model as geometry. **Texture mapping** addresses this by painting detail onto surfaces using two-dimensional images.

A texture map is an image indexed by coordinates $(u, v)$, typically ranging from 0 to 1. Each vertex of a mesh is assigned texture coordinates, and the rasterizer interpolates them across the triangle's interior to determine which texel (texture pixel) corresponds to each fragment. The simplest sampling method is **nearest-neighbor filtering**, which returns the texel closest to the computed coordinates, but this produces blocky artifacts. **Bilinear filtering** instead interpolates among the four nearest texels, producing smoother results. When a textured surface recedes into the distance, a single pixel may cover many texels, leading to aliasing artifacts — shimmering moiré patterns that distract the eye. **Mipmapping**, invented by **Lance Williams** in 1983, solves this by precomputing a hierarchy of successively downsampled versions of the texture and selecting the appropriate level based on the screen-space area of the fragment. **Trilinear filtering** interpolates between adjacent mipmap levels for seamless transitions, and **anisotropic filtering** further improves quality when the surface is viewed at oblique angles by sampling along the direction of greatest compression.

Beyond color, textures can encode geometric detail. **Bump mapping**, introduced by **Jim Blinn** in 1978, perturbs surface normals according to a height map without modifying the actual geometry, producing the illusion of small-scale relief at minimal cost. **Normal mapping** extends this idea by storing precomputed perturbed normals directly in a texture, allowing complex surface detail from high-resolution models to be transferred onto low-polygon meshes. **Parallax mapping** and **displacement mapping** go further still: parallax mapping offsets texture coordinates based on the viewing angle to simulate depth, while displacement mapping actually modifies the geometry, pushing vertices along their normals according to a height field.

**Procedural texturing** generates surface detail algorithmically rather than from stored images. The key building block is **Perlin noise**, developed by **Ken Perlin** in 1983 for the film *Tron*. Perlin noise produces smooth, natural-looking pseudo-random patterns by interpolating gradient values defined on a regular grid. By layering multiple octaves of noise at different frequencies and amplitudes — a technique called **fractal Brownian motion** — artists and programmers can synthesize convincing marble, clouds, terrain, and fire without any input image at all. Perlin received a Technical Achievement Academy Award in 1997 for this work, a testament to its transformative impact on the industry.

## Ray Tracing and Global Illumination

While rasterization excels at speed, it fundamentally operates one triangle at a time and struggles with effects that require knowledge of the entire scene — reflections, refractions, shadows, and indirect lighting. **Ray tracing** takes the opposite approach: it operates one ray at a time, following the path of light from the camera through each pixel into the scene.

The basic algorithm, described by **Turner Whitted** in his seminal 1980 paper, casts a **primary ray** from the camera through each pixel. When the ray strikes a surface, the algorithm spawns secondary rays: **shadow rays** toward each light source to determine visibility, **reflection rays** for mirror-like surfaces, and **refraction rays** for transparent materials. Each secondary ray may itself spawn further rays, producing a recursive tree of light interactions. The key computational primitive is the **ray-object intersection test** — determining where a ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ (with origin $\mathbf{o}$ and direction $\mathbf{d}$) hits a geometric shape. For a sphere of center $\mathbf{c}$ and radius $r$, this reduces to solving the quadratic equation $|\mathbf{o} + t\mathbf{d} - \mathbf{c}|^2 = r^2$. For triangles, the **Moller-Trumbore algorithm** efficiently computes both the intersection parameter $t$ and the barycentric coordinates using a single matrix-vector computation.

Naively testing every ray against every object in the scene yields $O(n)$ cost per ray, which is impractical for complex scenes. **Acceleration structures** reduce this to $O(\log n)$ expected time. The most common structure is the **bounding volume hierarchy (BVH)**, which recursively partitions objects into groups enclosed by axis-aligned bounding boxes. A ray traverses the tree top-down, testing against bounding boxes first and only examining leaf-level geometry when a box is hit. **KD-trees** and **octrees** partition space itself rather than objects, offering complementary trade-offs in construction time and traversal efficiency.

Whitted-style ray tracing handles specular reflections and refractions beautifully but cannot capture the subtle interplay of diffuse indirect lighting — the way light bounces off a red wall and tints a nearby white surface with a warm glow. This is the domain of **global illumination**, governed by the **rendering equation**, formulated by **James Kajiya** in 1986:

$$L_o(\mathbf{x}, \omega_o) = L_e(\mathbf{x}, \omega_o) + \int_{\Omega} f_r(\mathbf{x}, \omega_i, \omega_o) \, L_i(\mathbf{x}, \omega_i) \, (\omega_i \cdot \mathbf{n}) \, d\omega_i$$

This integral equation states that the outgoing radiance $L_o$ at a surface point $\mathbf{x}$ in direction $\omega_o$ equals the emitted radiance $L_e$ plus the integral over the hemisphere $\Omega$ of all incoming radiance $L_i$, weighted by the BRDF $f_r$ and the cosine of the incidence angle. The rendering equation is recursive — the incoming radiance at one point is the outgoing radiance of another — and has no closed-form solution for general scenes.

**Monte Carlo path tracing** solves the rendering equation by statistical sampling. Instead of integrating over the entire hemisphere, the algorithm randomly selects a direction $\omega_i$ according to some probability distribution, traces a ray in that direction, and weights the result by the ratio of the integrand to the probability density. By averaging many such samples, the estimate converges to the true value. The variance of this estimator — visible as noise in the image — decreases as $1/\sqrt{N}$ with the number of samples $N$. **Importance sampling** dramatically reduces variance by choosing sample directions that are more likely to contribute significantly to the integral, such as directions aligned with the BRDF lobe or toward bright light sources. **Multiple importance sampling (MIS)**, introduced by **Eric Veach** in 1995, combines samples from different distributions using a weighting scheme that provably minimizes variance. More advanced techniques include **photon mapping**, which pre-scatters photons from light sources into the scene and gathers them at shading points to capture caustics and diffuse interreflection, and **bidirectional path tracing**, which traces paths from both the camera and the lights and connects them, efficiently handling light paths that neither strategy alone would discover.

## Curves, Surfaces, and Geometric Modeling

Rendering algorithms need geometry to work with, and the mathematical representation of shape is itself a rich topic. **Parametric curves and surfaces** — smooth, analytically defined shapes controlled by a small number of points — are fundamental to computer-aided design, animation, and font rendering.

A **Bezier curve** of degree $n$ is defined by $n + 1$ **control points** $\mathbf{P}_0, \ldots, \mathbf{P}_n$ and the parameterization:

$$\mathbf{C}(t) = \sum_{i=0}^{n} \binom{n}{i} (1-t)^{n-i} t^i \, \mathbf{P}_i, \quad t \in [0, 1]$$

The functions $B_i^n(t) = \binom{n}{i}(1-t)^{n-i}t^i$ are the **Bernstein basis polynomials**. Bezier curves pass through their first and last control points and are tangent to the control polygon at the endpoints, but they do not generally pass through the interior control points — the control points exert a "gravitational pull" on the curve. **Pierre Bezier** at Renault and **Paul de Casteljau** at Citroen independently developed these curves in the early 1960s for automobile body design, though de Casteljau's work remained proprietary and unpublished for years.

For modeling complex shapes, a single high-degree Bezier curve is unwieldy; instead, multiple low-degree curves are joined together. **B-spline curves** provide a principled framework for this, offering local control (moving one control point affects only a limited portion of the curve) and guaranteed smoothness at the joints. A B-spline of degree $p$ with control points $\mathbf{P}_0, \ldots, \mathbf{P}_n$ and a **knot vector** $\{t_0, t_1, \ldots, t_{n+p+1}\}$ is defined by:

$$\mathbf{C}(t) = \sum_{i=0}^{n} N_{i,p}(t) \, \mathbf{P}_i$$

where $N_{i,p}(t)$ are the B-spline basis functions, computed recursively via the **Cox-de Boor** formula. **NURBS** (Non-Uniform Rational B-Splines) generalize B-splines by adding weights to each control point, enabling exact representation of conic sections — circles, ellipses, parabolas — that polynomial splines can only approximate. NURBS are the standard representation in industrial CAD software and are central to the mathematical field of Computer Aided Geometric Design.

Surfaces extend these ideas to two parameters. A **tensor product surface** $\mathbf{S}(u, v)$ applies the curve construction independently in each parameter direction, creating a patch controlled by a grid of control points. **Subdivision surfaces**, introduced by **Edwin Catmull** and **Jim Clark** in 1978 and independently by **Daniel Doo** and **Malcolm Sabin**, take a different approach: they define smooth surfaces as the limit of a recursive refinement process applied to a polygonal mesh. Starting from a coarse mesh, each subdivision step inserts new vertices and repositions existing ones according to fixed rules, producing a progressively smoother surface. Catmull-Clark subdivision generalizes to meshes with arbitrary topology (including extraordinary vertices where more or fewer than four edges meet), making it the preferred method for character and creature modeling in film and games. **Pixar** adopted subdivision surfaces as a core technology, and their use in *Geri's Game* (1997) earned a Technical Achievement Oscar.

## Animation, Simulation, and Procedural Generation

Computer graphics is not only about still images — it is about motion. **Animation** gives life to geometric models by specifying how their properties change over time, while **physics simulation** makes that motion obey the laws of the physical world.

The simplest animation technique is **keyframe interpolation**: an animator specifies the state of the scene at a few key moments (keyframes), and the system interpolates between them. Linear interpolation produces mechanical, robotic motion; **cubic spline interpolation** — using Hermite, Catmull-Rom, or Bezier curves in time — yields smooth trajectories with controllable velocity through **ease-in** and **ease-out** curves. For articulated characters, the skeleton is represented as a **bone hierarchy**, a tree of rigid transformations. **Forward kinematics** computes the position of each bone from the joint angles propagated down the hierarchy. The inverse problem — **inverse kinematics** — computes joint angles that place an end effector (a hand, a foot) at a desired position, typically solved by iterative numerical methods like the Jacobian transpose or cyclic coordinate descent. **Skinning** deforms the character's mesh by blending the transformations of nearby bones, weighted per vertex, to produce smooth deformation at joints.

**Physics simulation** replaces hand-authored motion with the output of differential equations. **Particle systems**, introduced by **William Reeves** at Lucasfilm in 1983 for the Genesis sequence in *Star Trek II: The Wrath of Khan*, model phenomena like fire, smoke, and explosions as collections of small particles, each governed by Newton's second law $\mathbf{F} = m\mathbf{a}$. **Rigid body dynamics** extends this to solid objects with orientation, angular velocity, and an **inertia tensor** that describes how mass is distributed. Collision detection — determining when and where objects touch — is a major computational challenge, addressed by spatial data structures (BVHs, spatial hashing) and algorithms like the **GJK algorithm** for convex shapes. **Soft body simulation** models deformable objects using mass-spring systems or finite element methods, while **cloth simulation** handles the bending, stretching, and collision of fabrics. **Fluid simulation** tackles liquids and gases, often using either grid-based **Eulerian** methods that solve the Navier-Stokes equations on a fixed grid, or particle-based **Lagrangian** methods like Smoothed Particle Hydrodynamics (SPH). Numerical integration advances these simulations through time; **Verlet integration** is popular for its simplicity and stability, while implicit methods like **backward Euler** handle the stiffness that arises in cloth and elastic materials.

**Procedural generation** creates content algorithmically, enabling vast, detailed worlds without manual authoring. Perlin noise and its variants drive terrain generation, where layered noise octaves produce mountain ranges, valleys, and coastlines. **L-systems**, formalized by **Aristid Lindenmayer** in 1968 for modeling plant growth, generate branching structures through string rewriting rules — a formal grammar whose productions encode botanical knowledge. Procedural techniques appear throughout modern games and simulations, generating not just geometry but textures, weather, ecosystems, and entire game worlds on the fly.

## Real-Time Rendering and Modern GPU Architecture

The demands of interactive applications — games, simulations, virtual reality — have driven the evolution of specialized hardware that executes the graphics pipeline at extraordinary speed. Modern **graphics processing units (GPUs)** are massively parallel processors containing thousands of simple cores organized into streaming multiprocessors, designed to execute the same program (a shader) on many data elements simultaneously. This **SIMD** (single instruction, multiple data) architecture mirrors the inherent parallelism of the graphics pipeline, where the same vertex or fragment shader must be applied independently to millions of primitives per frame.

**Deferred rendering** restructures the pipeline to separate geometry processing from lighting. In a first pass, all geometry is rendered into a set of **G-buffers** storing per-pixel normals, positions, albedo, and material properties. In a second pass, lighting is computed by reading from these buffers, allowing hundreds of dynamic lights without re-rendering geometry. This approach, impractical when GPU memory was scarce, became standard as memory capacities grew.

The pursuit of visual quality at interactive frame rates has produced a rich toolkit of approximation techniques. **Screen-space ambient occlusion (SSAO)** approximates the darkening that occurs in corners and crevices by sampling the depth buffer around each pixel. **Level of detail (LOD)** reduces geometric complexity for distant objects, seamlessly transitioning between mesh resolutions. **Temporal anti-aliasing (TAA)** combines information from multiple frames to smooth jagged edges without the cost of supersampling. **Variable rate shading** executes the fragment shader at different rates across the screen, devoting full resolution to areas of high detail and reducing it in peripheral or low-contrast regions — a technique particularly important for virtual reality, where the user's gaze is focused on a small area.

The advent of hardware-accelerated ray tracing, introduced with **NVIDIA's RTX** architecture in 2018, has blurred the traditional boundary between rasterization and ray tracing. Modern GPUs contain dedicated **ray tracing cores** that accelerate BVH traversal and ray-triangle intersection, enabling real-time reflections, global illumination, and shadows computed by ray tracing while the rest of the scene is rasterized. Hybrid rendering pipelines — rasterizing primary visibility and ray tracing secondary effects — represent the current state of the art, and the trajectory suggests that real-time rendering will increasingly converge with the physically-based techniques that have long been the domain of offline production rendering.

## Visualization, Virtual Reality, and Frontiers

Computer graphics extends beyond entertainment into scientific discovery and immersive experience. **Scientific visualization** transforms abstract numerical data — fluid flows, molecular structures, medical scans — into visual representations that the human eye and brain can interpret. **Volume rendering** displays three-dimensional scalar fields by casting rays through volumetric data and accumulating color and opacity according to a **transfer function** that maps data values to visual properties. The **marching cubes** algorithm, published by **William Lorensen** and **Harvey Cline** in 1987, extracts polygonal isosurfaces from volumetric data and remains one of the most widely cited papers in computer graphics. **Information visualization** applies graphic design principles to abstract data — networks, hierarchies, temporal patterns — enabling exploratory analysis and communication of complex datasets.

**Virtual and augmented reality** represent the most immersive applications of computer graphics. VR systems render stereoscopic images at high frame rates (90 Hz or more) with low latency to create a convincing sense of presence in a synthetic environment. **Foveated rendering**, guided by eye tracking, concentrates rendering effort where the user is looking, dramatically reducing computational cost. AR systems overlay synthetic imagery onto the real world, requiring real-time camera tracking, environment mapping, and correct handling of occlusion and lighting to maintain the illusion that virtual objects inhabit physical space. The optical, perceptual, and rendering challenges of these systems draw together nearly every topic in computer graphics — transformations, shading, acceleration structures, temporal coherence, and human factors.

At the research frontier, **neural graphics** is reshaping the field. **Neural radiance fields (NeRFs)**, introduced in 2020, represent scenes as continuous volumetric functions parameterized by neural networks, producing photorealistic novel views from a sparse set of photographs. **Differentiable rendering** makes the entire rendering pipeline differentiable, enabling gradient-based optimization of scene parameters — geometry, materials, lighting — from image observations. **Gaussian splatting** offers a point-based alternative to NeRFs with real-time rendering capability. These developments, which lie at the intersection of computer graphics and machine learning, suggest that the future of the field will be defined as much by learned representations as by the hand-crafted algorithms that have served it for sixty years.
