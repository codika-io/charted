---
title: "Quantum Computing"
description: "Computation using quantum mechanical phenomena — qubits, quantum gates, algorithms, error correction, and quantum advantage."
parent: computer-science/applied-and-interdisciplinary
order: 2
color: "#3b82f6"
difficulty: advanced
prerequisites: ["computer-science/theoretical-foundations/theory-of-computation"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Quantum computing harnesses the principles of quantum mechanics — superposition, entanglement, and interference — to process information in ways that have no classical analogue. It represents not merely a faster kind of computer but a fundamentally different model of computation, one that challenges our understanding of what problems are efficiently solvable.

## Foundations of Quantum Information

The basic unit of quantum information is the **qubit**, a two-level quantum system whose state is described by a vector in a two-dimensional complex Hilbert space $\mathcal{H} = \mathbb{C}^2$. Unlike a classical bit, which is either 0 or 1, a qubit can exist in a **superposition** of the two **computational basis states** $|0\rangle$ and $|1\rangle$. An arbitrary single-qubit state is written as $|\psi\rangle = \alpha|0\rangle + \beta|1\rangle$, where $\alpha$ and $\beta$ are complex numbers called **amplitudes** satisfying the normalization condition $|\alpha|^2 + |\beta|^2 = 1$. The geometric interpretation of a single qubit is the **Bloch sphere**: every pure state corresponds to a point on the surface of a unit sphere, with $|0\rangle$ at the north pole and $|1\rangle$ at the south pole, and the relative phase between $\alpha$ and $\beta$ determining the longitude.

When we **measure** a qubit in the computational basis, the superposition collapses: the outcome is 0 with probability $|\alpha|^2$ and 1 with probability $|\beta|^2$, and the post-measurement state is the corresponding basis state. This probabilistic collapse is governed by the **Born rule** and is one of the features that makes quantum computing both powerful and subtle — information is encoded in the amplitudes, but measurement irreversibly destroys the superposition.

A system of $n$ qubits lives in the tensor product space $(\mathbb{C}^2)^{\otimes n}$, which has dimension $2^n$. An arbitrary $n$-qubit state is $|\psi\rangle = \sum_{x \in \{0,1\}^n} \alpha_x |x\rangle$ with $\sum_x |\alpha_x|^2 = 1$, requiring $2^n$ complex amplitudes to specify. This exponential growth in the state space is the fundamental source of quantum computing's potential power. However, not all $n$-qubit states are equally interesting. **Product states** of the form $|\psi_1\rangle \otimes |\psi_2\rangle \otimes \cdots \otimes |\psi_n\rangle$ can be described with only $2n$ parameters, while **entangled states** — those that cannot be written as such a product — exhibit correlations that have no classical explanation.

**Quantum entanglement**, first highlighted as paradoxical by **Albert Einstein**, **Boris Podolsky**, and **Nathan Rosen** in their famous 1935 paper, is a physical resource that quantum algorithms exploit extensively. The simplest entangled states are the four **Bell states**, such as $|\Phi^+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)$, in which measuring one qubit instantly determines the outcome of measuring the other, regardless of the spatial separation between them. The **GHZ state** $\frac{1}{\sqrt{2}}(|000\rangle + |111\rangle)$ and the **W state** $\frac{1}{\sqrt{3}}(|001\rangle + |010\rangle + |100\rangle)$ generalize entanglement to three or more qubits, with qualitatively different entanglement properties. Quantifying entanglement — through measures like **entanglement entropy**, defined as the von Neumann entropy of the reduced density matrix of a subsystem — remains an active area of research in quantum information theory.

## Quantum Gates and Circuit Model

Quantum computation proceeds by applying **quantum gates** — unitary transformations — to qubits. The requirement of unitarity ($U^\dagger U = I$) ensures that quantum evolution is reversible, a fundamental constraint that distinguishes quantum gates from classical logic gates (most of which are irreversible and lose information). The **circuit model** of quantum computation, analogous to classical Boolean circuits, represents algorithms as sequences of gates applied to a register of qubits, followed by measurement.

The most important **single-qubit gates** include the **Pauli gates** $X$, $Y$, and $Z$ (the quantum analogues of classical NOT, with $X|0\rangle = |1\rangle$ and $X|1\rangle = |0\rangle$), the **Hadamard gate** $H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$ which creates an equal superposition $H|0\rangle = \frac{1}{\sqrt{2}}(|0\rangle + |1\rangle)$, and the **phase gates** $S$ and $T$ which introduce phase shifts of $\pi/2$ and $\pi/4$ respectively. General single-qubit rotations $R_x(\theta)$, $R_y(\theta)$, $R_z(\theta)$ rotate the Bloch sphere about the corresponding axis.

The essential **multi-qubit gate** is the **CNOT (Controlled-NOT)**, which flips the target qubit if and only if the control qubit is $|1\rangle$. The CNOT is the quantum workhorse for creating entanglement: applying a Hadamard to one qubit followed by a CNOT produces a Bell state from $|00\rangle$. Other important multi-qubit gates include the **Controlled-Z** gate, the **SWAP** gate, and the **Toffoli gate** (a doubly controlled NOT), which is universal for classical reversible computation. A key result in quantum computing is the **Solovay-Kitaev theorem**, which guarantees that any single-qubit unitary can be approximated to precision $\epsilon$ using $O(\log^c(1/\epsilon))$ gates from a fixed finite gate set (such as $\{H, T, \text{CNOT}\}$), establishing the **universality** of such gate sets for quantum computation.

Quantum circuits are drawn as wire diagrams: horizontal lines represent qubits, and gates are boxes or symbols placed on the wires. **Circuit depth** (the number of sequential gate layers) and **gate count** are the primary complexity measures. A major engineering challenge is that real quantum gates are imperfect — each gate introduces a small error — so minimizing circuit depth is crucial for near-term devices.

## Quantum Teleportation and Key Distribution

Before the great algorithmic results, quantum information theory produced two foundational protocols that demonstrated the power of entanglement as a resource. **Quantum teleportation**, discovered by **Charles Bennett**, **Gilles Brassard**, and collaborators in 1993, allows the transfer of an unknown quantum state from one location to another using a shared Bell pair and two classical bits of communication. The sender performs a **Bell measurement** on the unknown state and her half of the entangled pair, obtaining one of four outcomes. She transmits these two classical bits to the receiver, who applies the corresponding Pauli correction to his half of the pair, recovering the original state exactly. Crucially, teleportation does not violate the **no-cloning theorem** (which forbids copying an arbitrary quantum state) because the original state is destroyed in the Bell measurement. Nor does it enable faster-than-light communication, since the classical bits must travel through a conventional channel.

**Quantum key distribution (QKD)** uses quantum mechanics to establish shared secret keys with security guaranteed by the laws of physics rather than computational assumptions. The **BB84 protocol**, proposed by **Charles Bennett** and **Gilles Brassard** in 1984, has Alice send qubits encoded in randomly chosen bases (rectilinear or diagonal). Bob measures each qubit in a randomly chosen basis. Afterward, they publicly compare their basis choices and discard cases where they disagreed, leaving a shared raw key. Any eavesdropper (Eve) attempting to intercept and measure the qubits inevitably disturbs them — a consequence of the quantum no-cloning theorem and the disturbance caused by measurement — introducing detectable errors. The **E91 protocol**, proposed by **Artur Ekert** in 1991, achieves the same goal using entangled Bell pairs and the violation of **Bell inequalities** to certify the absence of eavesdropping. Modern research pursues **device-independent QKD**, which provides security even if the quantum devices themselves are untrusted, and practical **quantum networks** that use quantum repeaters and entanglement swapping to distribute entanglement over long distances.

## Landmark Quantum Algorithms

The theoretical case for quantum computing rests on a small number of algorithms that demonstrate exponential or polynomial speedups over the best known classical methods. The earliest example is the **Deutsch-Jozsa algorithm** (1992), which determines whether a Boolean function $f: \{0,1\}^n \to \{0,1\}$ is constant or balanced (promised to be one or the other) using a single quantum query, whereas any classical deterministic algorithm requires $2^{n-1} + 1$ queries in the worst case. While the problem is artificial, the algorithm introduced the key technique of **quantum parallelism**: preparing a uniform superposition of all inputs, applying the function as a quantum oracle, and using interference to extract global properties of $f$.

**Simon's algorithm** (1994), by **Daniel Simon**, solved a related problem — finding a hidden bitstring $s$ such that $f(x) = f(y)$ if and only if $x \oplus y \in \{0^n, s\}$ — with $O(n)$ quantum queries versus $\Omega(2^{n/2})$ classically. Simon's algorithm was the direct inspiration for the most celebrated quantum algorithm of all: **Shor's algorithm** for integer factoring. Published by **Peter Shor** in 1994, it reduces factoring to **order finding** (computing the smallest $r$ such that $a^r \equiv 1 \pmod{N}$) and solves order finding efficiently using the **Quantum Fourier Transform (QFT)**. The QFT maps a quantum state $|j\rangle$ to $\frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} e^{2\pi i jk/N} |k\rangle$ and can be implemented with $O(n^2)$ gates on $n$ qubits, exponentially faster than the classical Fast Fourier Transform applied to the same-size vector. After applying modular exponentiation and the QFT, measurement yields an approximation to a multiple of $1/r$, from which $r$ is extracted using **continued fractions**. The total running time is $O(n^3)$ for an $n$-bit integer, compared to the sub-exponential $O(e^{n^{1/3}})$ of the best classical algorithm (the general number field sieve). The implications for cryptography are profound: a sufficiently large quantum computer running Shor's algorithm would break RSA, Diffie-Hellman, and elliptic curve cryptography.

**Grover's algorithm** (1996), by **Lov Grover**, addresses unstructured search: given an oracle that marks one item among $N$, Grover's algorithm finds it with $O(\sqrt{N})$ queries, a quadratic speedup over the classical $O(N)$. The algorithm works by repeatedly applying an **amplitude amplification** operator — the oracle followed by a "diffusion" operator that reflects amplitudes about their mean — which geometrically rotates the state vector in a two-dimensional subspace toward the marked state. After approximately $\frac{\pi}{4}\sqrt{N}$ iterations, the marked state has near-unit amplitude. **Bennett, Bernstein, Brassard, and Vazirani** proved that this is optimal: any quantum algorithm for unstructured search requires $\Omega(\sqrt{N})$ queries.

## Quantum Error Correction

Real quantum hardware is inherently noisy. Qubits interact with their environment, causing **decoherence** — the loss of quantum coherence that turns superpositions into classical mixtures. Individual gate operations have error rates typically between $10^{-2}$ and $10^{-4}$, far too high for algorithms requiring millions of gates. **Quantum error correction (QEC)** addresses this by encoding a single **logical qubit** into multiple **physical qubits**, such that errors can be detected and corrected without measuring (and thereby destroying) the encoded information.

The simplest example is the **three-qubit bit-flip code**, which encodes $|0\rangle \to |000\rangle$ and $|1\rangle \to |111\rangle$. A single bit-flip error on any qubit can be detected by measuring parity checks (called **syndromes**) and corrected by majority vote, without ever measuring the logical state. A similar three-qubit code handles phase-flip errors. The **nine-qubit Shor code** (proposed by Peter Shor in 1995) combines both to correct arbitrary single-qubit errors, and the general theory of **stabilizer codes**, developed by **Daniel Gottesman**, provides a systematic framework based on the **Pauli group** for constructing and analyzing quantum error-correcting codes.

The most promising codes for large-scale quantum computing are **surface codes**, a family of topological codes in which physical qubits are arranged on a two-dimensional lattice and logical qubits are encoded in topological properties of the lattice. Surface codes have a high **threshold error rate** (around 1%, meaning that if individual gate errors are below this threshold, arbitrarily long computations can be performed reliably) and require only nearest-neighbor interactions, making them well-suited to superconducting qubit architectures. The price is a large overhead: encoding a single logical qubit with surface codes currently requires on the order of a thousand physical qubits. The **threshold theorem**, proved independently by several groups in the late 1990s, guarantees that fault-tolerant quantum computation is possible in principle: provided the physical error rate is below a constant threshold, an arbitrary quantum computation can be performed with polynomial overhead in the number of qubits and gates.

## Quantum Complexity and the NISQ Era

Quantum computing introduces new **complexity classes** that refine our understanding of computational difficulty. The central class is **BQP (Bounded-Error Quantum Polynomial Time)** — the set of decision problems solvable by a quantum computer in polynomial time with error probability at most 1/3. It is known that $\text{P} \subseteq \text{BQP} \subseteq \text{PSPACE}$, and it is believed (but not proven) that BQP is strictly larger than P and that BQP does not contain NP. Shor's algorithm places integer factoring in BQP, providing evidence that BQP extends beyond what classical computers can efficiently solve, since factoring is not known to be in P. The class **QMA (Quantum Merlin-Arthur)** is the quantum analogue of NP: a quantum verifier receives a quantum proof (a "witness" state) and must determine membership in the language. The canonical QMA-complete problem is the **Local Hamiltonian problem** — determining whether the ground-state energy of a local Hamiltonian is below a threshold — a quantum generalization of constraint satisfaction.

In the near term, we are in what **John Preskill** dubbed the **NISQ (Noisy Intermediate-Scale Quantum)** era: devices with tens to hundreds of qubits, limited connectivity, and error rates too high for full fault tolerance. NISQ algorithms are designed to extract useful computation from shallow, noisy circuits. The most prominent are **variational quantum algorithms**, which use a classical optimizer to tune the parameters of a short quantum circuit. The **Variational Quantum Eigensolver (VQE)**, proposed by **Alberto Peruzzo** and collaborators in 2014, estimates the ground-state energy of a molecular Hamiltonian by minimizing the expectation value $\langle \psi(\theta) | H | \psi(\theta) \rangle$ over parametrized circuits $|\psi(\theta)\rangle$. The **Quantum Approximate Optimization Algorithm (QAOA)**, introduced by **Edward Farhi**, **Jeffrey Goldstone**, and **Sam Gutmann** in 2014, applies alternating layers of problem and mixer Hamiltonians to approximately solve combinatorial optimization problems like MaxCut. Both face challenges: **barren plateaus** in the optimization landscape can make gradient-based training exponentially hard for deep circuits, and demonstrating a clear quantum advantage over classical heuristics for practical problem sizes remains elusive.

## Physical Implementations and the Road Ahead

Building a quantum computer requires physical systems that can maintain quantum coherence, be initialized to known states, undergo controlled unitary evolution, and be measured reliably. Several leading platforms are competing toward scalable quantum computation.

**Superconducting qubits**, used by IBM, Google, and others, encode quantum information in the energy levels of superconducting circuits cooled to millikelvin temperatures. The **transmon** qubit — a charge qubit made insensitive to charge noise through a large shunting capacitance — is the current workhorse. In 2019, Google's Sycamore processor, with 53 superconducting qubits, performed a sampling task in 200 seconds that would have taken the best classical supercomputer an estimated 10,000 years, a milestone claimed as **quantum supremacy** (though IBM disputed the classical estimate). Superconducting systems offer fast gate times (tens of nanoseconds) but suffer from short coherence times (tens to hundreds of microseconds) and require complex cryogenic infrastructure.

**Trapped ion** systems, developed by groups including IonQ and Honeywell (now Quantinuum), use individual ions confined in electromagnetic traps, with qubit states encoded in electronic energy levels. Laser or microwave pulses implement gates. Trapped ions boast the highest gate fidelities of any platform (above 99.9% for single-qubit gates and above 99.5% for two-qubit gates) and **all-to-all connectivity** — any pair of ions in the trap can interact directly — eliminating the need for SWAP routing. Gate times are slower (microseconds) and scaling to large numbers of ions requires modular architectures with ion shuttling or photonic interconnects.

**Photonic quantum computing** uses single photons as qubits, with polarization or path encoding. Linear optical systems, combined with photon detectors and feedforward, can achieve universal quantum computation through the **KLM (Knill-Laflamme-Milburn)** scheme, though the resource overhead is substantial. Companies like PsiQuantum and Xanadu are pursuing photonic approaches, with advantages including room-temperature operation and natural compatibility with quantum networking. More speculative approaches include **topological quantum computing**, which would encode information in exotic quasiparticles called **anyons** whose braiding operations are inherently fault-tolerant, though the experimental realization of suitable anyonic systems (particularly **Majorana fermions** in semiconductor-superconductor heterostructures) remains a major open challenge.

The path from today's NISQ devices to large-scale, fault-tolerant quantum computers is uncertain. Current roadmaps from major hardware vendors project thousands of logical qubits within the next decade, which would be sufficient to run Shor's algorithm on cryptographically relevant problem sizes and to perform molecular simulations beyond classical reach. Whether this timeline proves accurate will depend on advances in qubit quality, error correction overhead reduction, and the development of quantum software and compilation tools that can bridge the gap between abstract algorithms and the messy realities of physical hardware.
