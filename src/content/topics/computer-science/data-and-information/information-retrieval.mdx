---
title: "Information Retrieval"
description: "The science of searching and organizing large collections of information — indexing, ranking, text processing, and web search."
parent: computer-science/data-and-information
order: 2
color: "#3b82f6"
difficulty: intermediate
prerequisites: ["computer-science/data-and-information/databases"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Information retrieval is the science of finding relevant material — usually textual documents, but increasingly images, audio, and structured data — within large, unstructured collections. It is the discipline that gave the world search engines, recommender systems, and question answering, and its ideas now pervade nearly every application that connects people with information. The field draws on databases for storage, on algorithms for efficiency, on statistics and linear algebra for ranking, and increasingly on machine learning for understanding meaning.

## Foundations and the Boolean Model

The roots of information retrieval reach back to the problem of organizing knowledge in libraries. **Calvin Mooers** coined the term "information retrieval" in 1950, but the field's modern computational foundations were laid in the 1960s and 1970s by **Gerard Salton** and his students at Cornell University. Salton's **SMART system** (System for the Mechanical Analysis and Retrieval of Text) was the first fully automatic information retrieval system and served as the primary research platform for the field for over two decades. Before SMART, most retrieval systems used the **Boolean retrieval model**, in which queries are expressed as Boolean combinations of terms — for example, "database AND (distributed OR replicated) AND NOT hierarchical" — and the system returns exactly those documents that satisfy the Boolean expression.

The Boolean model has the appeal of precision: it returns exactly the documents that match the logical specification, no more and no less. This makes it well-suited to professional contexts such as legal research and patent search, where users need exhaustive, reproducible results and are willing to craft complex queries. However, the Boolean model has severe limitations for general-purpose search. It provides no ranking — a document either matches or it does not, and there is no way to distinguish highly relevant matches from marginal ones. It also demands that users formulate precise Boolean expressions, which most people find unnatural. These limitations motivated the development of ranked retrieval models, beginning with Salton's own vector space model.

Before any retrieval model can operate, raw text must be transformed into a form suitable for indexing and matching. This **text preprocessing** pipeline typically includes **tokenization** (splitting text into individual terms), **case normalization**, **stopword removal** (discarding extremely common words such as "the" and "is" that carry little discriminative value), and **stemming** or **lemmatization** (reducing words to their root forms — for example, "running," "runs," and "ran" might all be reduced to "run"). The stemming algorithm most widely associated with the field is the **Porter stemmer**, published by Martin Porter in 1980, which applies a cascade of rule-based suffix-stripping transformations. Lemmatization, which uses morphological analysis to return actual dictionary forms, is more linguistically accurate but computationally heavier. The choices made at this stage have a significant impact on retrieval effectiveness, and there is no single configuration that is universally optimal.

## Inverted Indexes

The central data structure of information retrieval is the **inverted index**, the mechanism that makes it possible to search millions or billions of documents in milliseconds. An inverted index is conceptually simple: for every distinct term in the collection, it maintains a **posting list** — an ordered list of the identifiers of all documents containing that term, often augmented with the positions of each occurrence within the document and the term's frequency. The collection of all terms is stored in a **dictionary** (or lexicon), which maps each term to its posting list. Given a query term, the system looks up the term in the dictionary and retrieves the corresponding posting list; for multi-term queries, it intersects or unions the posting lists as needed.

The construction of an inverted index over a large document collection is itself a significant engineering challenge. The **single-pass in-memory indexing (SPIMI)** algorithm processes documents in blocks that fit in memory, building a separate dictionary and posting lists for each block, then merges the blocks into a single index on disk. For collections too large for a single machine, **distributed indexing** partitions the problem across many nodes — either by document (each node indexes a subset of documents and queries are broadcast to all nodes) or by term (each node is responsible for the posting lists of a subset of terms and receives only queries containing those terms). Web-scale search engines use a combination of both strategies, with multiple levels of sharding and replication.

Compression is essential for managing the size of inverted indexes. Posting lists are stored as sorted sequences of document identifiers, which means that the gaps between consecutive entries (the **d-gaps**) tend to be small and can be encoded efficiently using variable-length codes. **Variable-byte encoding** represents each integer using one or more bytes, with a continuation bit indicating whether more bytes follow. **Gamma coding** and **delta coding** use a unary prefix to encode the length of the binary representation of each gap. More sophisticated schemes such as **PForDelta** pack blocks of gaps into fixed-width frames with exceptions handled separately, achieving both high compression ratios and fast decompression. The dictionary itself is typically compressed using **front coding**, which exploits the alphabetical ordering of terms to store only the prefix shared with the previous term and the differing suffix.

## Ranked Retrieval and Scoring Models

The shift from Boolean retrieval to **ranked retrieval** — returning documents ordered by estimated relevance rather than as an unranked set — was the transformative development that made modern search possible. The foundation of ranked retrieval is the idea of assigning each document a numerical **relevance score** with respect to a query and returning the top-scoring documents.

The most influential scoring scheme is **TF-IDF** (term frequency-inverse document frequency), which Salton and his colleagues developed through the SMART experiments. The intuition is twofold: a term that appears frequently in a document is more likely to be relevant to a query containing that term (**term frequency**, TF), but a term that appears in many documents across the collection is less discriminative and should be down-weighted (**inverse document frequency**, IDF). The IDF of a term $t$ in a collection of $N$ documents, where $t$ appears in $\text{df}_t$ documents, is typically defined as $\text{idf}(t) = \log(N / \text{df}_t)$. The TF-IDF weight of term $t$ in document $d$ is then $\text{tf-idf}(t, d) = \text{tf}(t, d) \cdot \text{idf}(t)$, where $\text{tf}(t, d)$ is the frequency of $t$ in $d$ (often with sublinear scaling such as $1 + \log(\text{tf})$ to prevent very long documents from dominating). Documents and queries are represented as vectors in a high-dimensional space where each dimension corresponds to a term, and relevance is measured by the **cosine similarity** between the query vector and the document vector — the cosine of the angle between them, which normalizes for document length.

The probabilistic approach to retrieval, developed by Stephen Robertson and Karen Sparck Jones in the 1970s, led to the **BM25** ranking function (BM stands for "Best Match"), which remains one of the most effective scoring functions despite being over three decades old. BM25 refines TF-IDF by incorporating a saturation function for term frequency (so that additional occurrences of a term contribute diminishing returns) and an explicit document length normalization parameter. For a query $Q$ containing terms $q_1, \ldots, q_n$, the BM25 score of document $d$ is:

$$\text{BM25}(d, Q) = \sum_{i=1}^{n} \text{idf}(q_i) \cdot \frac{\text{tf}(q_i, d) \cdot (k_1 + 1)}{\text{tf}(q_i, d) + k_1 \cdot \left(1 - b + b \cdot \frac{|d|}{\text{avgdl}}\right)}$$

where $k_1$ and $b$ are tunable parameters (typically $k_1 \approx 1.2$ and $b \approx 0.75$), $|d|$ is the length of document $d$, and $\text{avgdl}$ is the average document length in the collection. The **language modeling** approach to retrieval, introduced by Jay Ponte and W. Bruce Croft in 1998, takes a different perspective: it estimates the probability that a document's language model would generate the query, ranking documents by this query likelihood. Despite their different theoretical motivations, BM25 and query likelihood models often produce remarkably similar rankings in practice.

## Evaluation and Metrics

Rigorous evaluation is a defining characteristic of information retrieval as a discipline. The **Cranfield paradigm**, established by Cyril Cleverdon's experiments at the Cranfield Institute of Technology in the early 1960s, introduced the methodology of evaluating retrieval systems using standardized test collections consisting of a fixed set of documents, a set of queries (called **topics**), and human **relevance judgments** indicating which documents are relevant to each query. This paradigm was scaled up dramatically by the **Text REtrieval Conference (TREC)**, organized by the U.S. National Institute of Standards and Technology (NIST) since 1992, which has produced large, publicly available test collections across dozens of retrieval tasks and domains.

The two most fundamental metrics are **precision** and **recall**. Precision is the fraction of retrieved documents that are relevant: $P = |R \cap A| / |A|$, where $R$ is the set of relevant documents and $A$ is the set of retrieved documents. Recall is the fraction of relevant documents that are retrieved: $R = |R \cap A| / |R|$. These two metrics are in tension — improving one typically comes at the cost of the other — and various composite metrics have been developed to capture the trade-off. **Precision at rank $k$** ($P@k$) measures precision among only the top $k$ results, reflecting the practical reality that most users examine only the first page of search results. **Mean Average Precision (MAP)** computes the average precision at each rank position where a relevant document is found, then averages across all queries, providing a single-number summary that is sensitive to ranking quality across all recall levels.

**Normalized Discounted Cumulative Gain (NDCG)** addresses a limitation of precision and recall by accommodating graded relevance judgments (e.g., "highly relevant," "somewhat relevant," "not relevant") rather than binary ones. NDCG computes a score that rewards placing highly relevant documents at the top of the ranking, with a logarithmic discount for lower positions: $\text{DCG}@k = \sum_{i=1}^{k} \frac{2^{r_i} - 1}{\log_2(i + 1)}$, where $r_i$ is the relevance grade of the document at position $i$. This raw score is normalized by dividing by the ideal DCG (the DCG achieved by the perfect ranking), yielding a value between 0 and 1. **Mean Reciprocal Rank (MRR)**, used when only one relevant result matters (as in navigational queries), is the average of $1/\text{rank}$ of the first relevant document across all queries.

## Web Search and Link Analysis

The emergence of the World Wide Web in the 1990s transformed information retrieval from an academic specialty into a technology used by billions of people daily. Web search introduced challenges that the traditional IR community had not faced: the scale of the collection (billions of pages), the heterogeneity of content (from scholarly articles to spam), the lack of editorial control, and the need for real-time freshness. The solutions to these challenges gave rise to a new generation of search engines and, ultimately, to some of the most influential algorithms in computer science.

The most famous of these is **PageRank**, developed by **Larry Page** and **Sergey Brin** at Stanford University and described in their 1998 paper that also introduced the Google search engine. PageRank models the web as a directed graph where pages are nodes and hyperlinks are edges, and it defines the importance of a page as the probability that a random surfer, following links uniformly at random and occasionally jumping to a random page (with **damping factor** $\alpha$, typically 0.85), will be found at that page in the stationary distribution of the resulting Markov chain. Formally, the PageRank of page $i$ is:

$$\text{PR}(i) = \frac{1 - \alpha}{N} + \alpha \sum_{j \in B_i} \frac{\text{PR}(j)}{L_j}$$

where $N$ is the total number of pages, $B_i$ is the set of pages linking to $i$, and $L_j$ is the number of outgoing links from page $j$. This equation is solved iteratively (by power iteration) over the entire web graph. A complementary approach is the **HITS algorithm** (Hyperlink-Induced Topic Search), developed by Jon Kleinberg in 1999, which identifies two types of important pages for each query: **hubs** (pages that link to many relevant pages) and **authorities** (pages that are linked to by many hubs). Both PageRank and HITS demonstrated that the link structure of the web carries rich information about content quality and relevance that purely text-based methods cannot capture.

Modern web search engines combine hundreds of signals — text relevance (BM25 and its successors), link analysis, page quality, freshness, geographic and linguistic signals, user engagement data — into a unified ranking function. The architecture involves massive-scale **crawling** (systematically downloading web pages), **indexing** (building inverted indexes partitioned across thousands of machines), **query processing** (routing each query to the relevant index partitions and merging results), and **serving** (applying final re-ranking and personalization before presenting results to the user). The infrastructure required is staggering, but the underlying principles remain those of information retrieval.

## Relevance Feedback and Query Expansion

Users often struggle to express their information needs in a few query terms, and even well-formulated queries may miss relevant documents that use different vocabulary. **Relevance feedback** and **query expansion** are techniques for bridging this gap by automatically enriching or reformulating the query.

In **explicit relevance feedback**, introduced by Salton and refined by Rocchio in the 1970s, the user marks some retrieved documents as relevant or non-relevant, and the system adjusts the query vector by moving it toward the centroid of the relevant documents and away from the centroid of the non-relevant ones. The **Rocchio algorithm** formalizes this as $\vec{q'} = \alpha \vec{q} + \beta \frac{1}{|D_r|}\sum_{d \in D_r} \vec{d} - \gamma \frac{1}{|D_{nr}|}\sum_{d \in D_{nr}} \vec{d}$, where $D_r$ and $D_{nr}$ are the sets of relevant and non-relevant documents, and $\alpha$, $\beta$, $\gamma$ are tunable weights. **Pseudo-relevance feedback** (also called blind feedback) automates this process by assuming that the top-ranked documents from an initial retrieval are relevant and using them to expand the query without any user input. While pseudo-relevance feedback often improves recall, it can backfire when the initial results are poor, a phenomenon known as **query drift**.

**Query expansion** enriches the query with synonyms, related terms, or statistically associated words. Sources for expansion terms include hand-curated thesauri (such as WordNet), co-occurrence statistics computed from the document collection, and more recently, neural language models that can identify semantically similar terms. **Latent Semantic Analysis (LSA)**, introduced by Scott Deerwester and colleagues in 1990, uses singular value decomposition (SVD) to project the term-document matrix into a lower-dimensional space where synonymous terms and related concepts are mapped to nearby points. LSA was one of the earliest techniques to address the **vocabulary mismatch problem** — the fundamental difficulty that the same concept can be expressed using different words — and it laid the groundwork for the dense vector representations that now dominate modern retrieval.

## Learning to Rank and Neural Retrieval

The advent of machine learning transformed retrieval from a field dominated by hand-crafted scoring functions into one where ranking models are trained from data. **Learning to rank** emerged in the mid-2000s as a framework for training ranking functions that combine hundreds of features — text similarity scores, link analysis metrics, document quality signals, query-document interaction features — into a single relevance score.

Learning to rank approaches are categorized by their loss functions. **Pointwise** methods treat ranking as a regression or classification problem on individual documents. **Pairwise** methods, exemplified by RankSVM and RankBoost, optimize the ordering of document pairs — the loss is incurred when a less relevant document is ranked above a more relevant one. **Listwise** methods, such as **LambdaMART** (a gradient-boosted tree model that directly optimizes NDCG), operate on entire ranked lists. LambdaMART, developed by Christopher Burges and colleagues at Microsoft Research, became one of the most successful ranking algorithms in commercial search engines and remains a strong baseline in competitions.

The most dramatic recent transformation has been the application of deep learning to retrieval. **Dense passage retrieval (DPR)**, introduced by Vladimir Karpukhin and colleagues at Facebook AI Research in 2020, uses a **bi-encoder** architecture in which separate BERT-based encoders produce dense vector representations of queries and documents. Retrieval is performed by nearest-neighbor search in the embedding space, typically using approximate nearest-neighbor indexes such as HNSW or FAISS. The **ColBERT** model, developed by Omar Khattab and Matei Zaharia, introduced **contextualized late interaction**: instead of compressing each document into a single vector, ColBERT retains a vector for each token and computes relevance by matching query token vectors against document token vectors, achieving a balance between the efficiency of bi-encoders and the effectiveness of full cross-attention models. **Cross-encoder** re-rankers, which concatenate the query and document and pass them through a single transformer to produce a relevance score, achieve the highest accuracy but are too expensive for first-stage retrieval over large collections, so they are typically applied only to the top candidates identified by a faster first-stage model. **Hybrid retrieval** systems combine sparse methods (BM25) with dense methods (bi-encoder retrieval) and neural re-ranking, exploiting the complementary strengths of lexical matching and semantic understanding.

## Recommender Systems and Specialized Search

**Recommender systems** are a natural extension of information retrieval: instead of matching explicit queries to documents, they predict which items (products, movies, articles, songs) a user will find interesting based on past behavior and preferences. **Collaborative filtering**, the dominant paradigm, comes in two flavors. **User-based collaborative filtering** identifies users with similar preference histories and recommends items liked by similar users. **Item-based collaborative filtering**, which scales better, computes similarity between items based on patterns of user interaction and recommends items similar to those a user has already engaged with. Both approaches can be formalized through **matrix factorization**, in which the user-item interaction matrix is decomposed into low-rank factor matrices, with the latent factors representing hidden dimensions of taste. The **alternating least squares (ALS)** algorithm and its variants are widely used for this factorization. **Content-based recommendation** uses features of the items themselves (genre, author, description) to recommend similar items, avoiding the **cold-start problem** that plagues collaborative filtering when new users or items have no interaction history. Modern recommender systems are typically **hybrid**, combining collaborative and content-based signals and increasingly incorporating deep learning, knowledge graphs, and contextual information.

Beyond web search and recommendation, information retrieval techniques have been adapted to a wide range of specialized domains. **Medical information retrieval** must handle complex terminologies and the critical need for precision. **Legal document retrieval** requires exhaustive recall to ensure no relevant precedent is missed. **E-commerce product search** combines text matching with structured attributes (price, brand, ratings) and business objectives (revenue, inventory). **Question answering systems** go beyond document retrieval to extract or generate direct answers to natural language questions, using reading comprehension models built on transformer architectures. The boundaries between information retrieval and natural language processing have become increasingly blurred, as large language models are now used both for generating queries, understanding documents, and synthesizing answers from retrieved passages in what has come to be called **retrieval-augmented generation (RAG)**. The core principles of the field, however, remain constant: build effective representations of information needs and information objects, match them efficiently, rank the results by estimated relevance, and measure how well the system serves its users.
