---
title: "Parallel Computing"
description: "The theory and practice of concurrent and parallel computation — shared memory, message passing, GPUs, and high-performance computing."
parent: computer-science/systems
order: 6
color: "#3b82f6"
difficulty: advanced
prerequisites: ["computer-science/systems/computer-architecture", "computer-science/systems/operating-systems"]
status: draft
author: agent
lastEditedBy: agent
lastUpdated: "2026-02-28"
---

Parallel computing is the simultaneous use of multiple processing elements to solve a computational problem. Where sequential computing executes one operation at a time, parallel computing decomposes a problem into parts that can be worked on concurrently, reducing time-to-solution or enabling problems of a scale that no single processor could handle. Since the mid-2000s, when single-core clock frequencies hit physical limits, parallelism has become the primary mechanism for continued growth in computational power — making this subject essential not only for supercomputing specialists but for every programmer who writes software for modern hardware.

## Fundamentals of Parallelism

The theoretical foundation of parallel computing begins with two complementary laws that bound the achievable speedup. **Amdahl's law**, formulated by **Gene Amdahl** in 1967, states that if a fraction $f$ of a computation is inherently serial, the maximum speedup with $p$ processors is

$$S(p) = \frac{1}{f + \frac{1-f}{p}}$$

As $p \to \infty$, the speedup approaches $1/f$. A program with even 5 percent serial code can achieve at most 20x speedup, regardless of how many processors are available. Amdahl's law is a powerful argument for optimizing the serial portion of code and for choosing algorithms that minimize sequential dependencies.

**Gustafson's law** (1988) provides a more optimistic perspective by observing that in practice, users with more processors often solve larger problems rather than the same problem faster. If the parallel portion of the work scales with the problem size while the serial portion remains fixed, the speedup scales nearly linearly with the number of processors:

$$S(p) = f + p(1 - f)$$

This distinction between **strong scaling** (fixed problem size, increasing processors) and **weak scaling** (problem size proportional to processors) is fundamental to understanding when parallelism delivers value.

Parallelism manifests at multiple levels. **Instruction-level parallelism** (ILP) is exploited by the processor hardware itself, through pipelining, superscalar execution, and out-of-order scheduling, as discussed in Computer Architecture. **Data-level parallelism** applies the same operation to many data elements simultaneously — the domain of vector processors and GPUs. **Task-level parallelism** decomposes a computation into independent tasks that can execute concurrently on different processors. **Pipeline parallelism** chains stages of processing, with each stage operating on a different input item simultaneously.

Parallel architectures are classified by **Flynn's taxonomy** (1966): **SISD** (Single Instruction, Single Data — the classical sequential machine), **SIMD** (Single Instruction, Multiple Data — vector processors and GPU compute units), **MISD** (Multiple Instruction, Single Data — rare, sometimes used for fault-tolerant systems), and **MIMD** (Multiple Instruction, Multiple Data — multicore processors and clusters). The two dominant programming paradigms correspond to the two main MIMD memory architectures: **shared memory** (all processors access a common address space) and **distributed memory** (each processor has its own private memory and communicates with others by sending messages).

## Shared-Memory Parallelism and OpenMP

In a **shared-memory** system, all threads can read and write the same memory locations. This simplifies communication — threads exchange data simply by reading and writing shared variables — but introduces the challenge of **synchronization**: ensuring that concurrent accesses to shared data do not produce incorrect results. The **memory consistency model** defines the rules governing when a write by one thread becomes visible to others. **Sequential consistency**, defined by **Leslie Lamport** in 1979, requires that the result of any parallel execution be equivalent to some interleaving of the threads' operations in program order. Modern hardware relaxes this for performance, using weaker models (such as x86's Total Store Order or ARM's relaxed model) that allow certain reorderings, requiring programmers to use **memory fences** to enforce ordering when necessary.

**OpenMP** (Open Multi-Processing) is the dominant API for shared-memory parallel programming in C, C++, and Fortran. OpenMP uses compiler directives (pragmas) to annotate regions of code that should be executed in parallel. A parallel region creates a team of threads, and work-sharing constructs distribute iterations of loops or sections of code among them. Data-sharing clauses specify whether variables are **shared** (accessible to all threads), **private** (each thread gets its own copy), or **reduction** variables (where each thread computes a partial result that is combined at the end). Synchronization constructs — **barriers** (where all threads wait until every thread has arrived), **critical sections** (where only one thread may execute at a time), and **atomic operations** — prevent race conditions.

The choice of **scheduling strategy** for loop iterations affects load balance and cache behavior. **Static** scheduling divides iterations into contiguous blocks assigned to threads at compile time — simple and cache-friendly but potentially unbalanced if iterations have variable execution times. **Dynamic** scheduling assigns iterations to threads on demand from a shared work queue — better for irregular workloads but with higher overhead. **Guided** scheduling starts with large blocks and reduces the block size as work is consumed, balancing between the two.

Debugging shared-memory parallel programs is notoriously difficult. **Race conditions** — where the outcome depends on the unpredictable timing of thread execution — can produce bugs that are intermittent, non-reproducible, and devastating. Tools like **ThreadSanitizer** (a dynamic race detector) and **Helgrind** (from the Valgrind suite) instrument memory accesses to detect data races at runtime. **Deadlocks**, where threads wait in a cycle for resources held by each other, are another hazard; they can be avoided by establishing a global ordering on lock acquisition or detected by monitoring the resource-dependency graph.

## Message-Passing Parallelism and MPI

In a **distributed-memory** system, each processing element has its own private memory and communicates with others by explicitly sending and receiving messages. This model maps naturally to clusters of networked machines, where there is no shared physical memory, and it scales to hundreds of thousands of processors — the realm of high-performance computing (HPC).

The **Message Passing Interface** (MPI) is the standard API for distributed-memory parallel programming. MPI defines a rich set of communication primitives organized around **communicators** — groups of processes, each identified by a **rank** (an integer from 0 to $n-1$). **Point-to-point** communication between pairs of processes uses send and receive operations. **Blocking** sends and receives (MPI_Send, MPI_Recv) do not return until the operation is complete; **non-blocking** variants (MPI_Isend, MPI_Irecv) return immediately and allow computation to overlap with communication, with explicit wait or test operations to check for completion.

**Collective communication** involves all processes in a communicator. **Broadcast** (MPI_Bcast) sends data from one process to all others. **Scatter** distributes different portions of an array to different processes; **Gather** collects them back. **Reduce** (MPI_Reduce) combines values from all processes using an associative operation (sum, max, etc.) and delivers the result to one process; **All-reduce** delivers the result to all processes. **Scan** (prefix sum) computes partial reductions. These collective operations are implemented using tree-based, hypercube, or ring algorithms optimized for the underlying network topology, and they form the communication backbone of most parallel scientific applications.

Performance optimization in MPI programs centers on minimizing communication overhead and maximizing the overlap of computation with communication. The **communication-to-computation ratio** — how much time is spent exchanging messages relative to doing useful work — is the primary determinant of parallel efficiency. Strategies include batching small messages into larger ones (to amortize latency), using non-blocking communication to overlap with computation, and choosing data decompositions that minimize the surface area of inter-process boundaries (and thus the volume of data exchanged). **Process placement** — mapping MPI ranks to physical processors and network nodes — affects communication latency; MPI's virtual topology features (Cartesian and graph topologies) help align logical communication patterns with physical network structure.

## GPU Computing

**Graphics Processing Units** (GPUs) are massively parallel processors originally designed for rendering graphics but now widely used for general-purpose computation. A modern GPU contains thousands of simple cores organized into **streaming multiprocessors** (SMs), each executing groups of threads — called **warps** (NVIDIA) or **wavefronts** (AMD) — in lockstep SIMD fashion. This architecture delivers enormous throughput for data-parallel workloads where the same operation is applied independently to thousands or millions of data elements.

**CUDA** (Compute Unified Device Architecture), introduced by NVIDIA in 2006, is the dominant programming model for GPU computing. A CUDA program defines **kernels** — functions that execute on the GPU — and launches them with a specified grid of **thread blocks**. Each thread block runs on a single SM and can use fast **shared memory** (a programmer-managed scratchpad, typically 48-96 KB per SM) for inter-thread communication. Threads within a block can synchronize via barriers; threads in different blocks run independently. The hierarchy of threads, blocks, and grids allows a single kernel launch to utilize the entire GPU.

The GPU memory hierarchy is critical to performance. **Global memory** (several to tens of gigabytes of high-bandwidth DRAM) is accessible to all threads but has high latency (hundreds of cycles). **Shared memory** is on-chip and fast (a few cycles) but small and shared among threads in a block. **Registers** provide the fastest storage but are strictly private to each thread. Achieving high performance requires careful attention to **memory coalescing** — ensuring that threads in a warp access consecutive memory addresses, so that the hardware can combine their requests into a single wide memory transaction. **Bank conflicts** in shared memory (when multiple threads access the same memory bank simultaneously) cause serialization and must be avoided through careful data layout. **Occupancy** — the ratio of active warps to the maximum number of warps an SM can support — affects the GPU's ability to hide memory latency by switching between warps.

**OpenCL** (Open Computing Language) provides a portable alternative to CUDA, supporting GPUs from multiple vendors as well as CPUs, FPGAs, and other accelerators. OpenCL's programming model is similar to CUDA — work-items, work-groups, and an ND-range correspond to CUDA threads, thread blocks, and grids — but the added portability comes with some performance overhead and a more complex API. For applications requiring the highest GPU performance, CUDA remains the standard; for cross-platform applications, OpenCL provides breadth.

## Parallel Algorithm Design

Designing efficient parallel algorithms requires rethinking problems from the ground up, not merely adding parallelism to sequential solutions. The most fundamental parallel primitive is the **parallel reduction**: computing the sum (or max, or any associative operation) of $n$ values in $O(\log n)$ steps using $n/2$ processors, compared to $O(n)$ steps sequentially. The closely related **parallel prefix sum** (scan) computes all partial sums $[a_1, a_1+a_2, a_1+a_2+a_3, \ldots]$ in $O(\log n)$ steps, and it is a building block for an astonishing variety of parallel algorithms — from stream compaction (extracting elements satisfying a predicate) to radix sort to sparse matrix operations.

**Parallel sorting** algorithms include **bitonic sort**, which constructs a sequence of compare-and-swap operations that can be performed in parallel, achieving $O(\log^2 n)$ parallel time; and **sample sort**, which partitions the input into roughly equal-sized buckets by choosing a set of splitter elements, sorts within each bucket independently, and achieves near-optimal parallel efficiency for large datasets. Parallel sorting is essential in database systems, where sort-merge joins and order-by operations must process terabytes of data.

**Parallel graph algorithms** pose special challenges because graph structure often leads to irregular memory access patterns and unpredictable workloads. Parallel **breadth-first search** (BFS) is a fundamental building block, typically implemented as a frontier-based algorithm where each level of the BFS tree is explored in parallel. Parallel shortest-path algorithms (Bellman-Ford parallelizes naturally across edges; Dijkstra's greedy structure is harder to parallelize), minimum spanning tree algorithms, and connected components algorithms are active areas of research with practical importance in social network analysis, bioinformatics, and route planning.

**Domain decomposition** is the standard technique for parallelizing scientific simulations. The computational domain — a physical space discretized into a grid — is partitioned among processors, with each processor responsible for updating its subdomain. Neighboring subdomains must exchange boundary data (so-called **ghost cells** or **halo exchanges**) at each time step. The art of domain decomposition lies in balancing the computational load across processors while minimizing the surface area of inter-partition boundaries (and thus the communication volume). For structured grids, simple block or cyclic decompositions often suffice; for unstructured meshes, graph partitioning algorithms like METIS provide high-quality decompositions.

## Performance Analysis and the Roofline Model

Rigorous performance analysis is essential for understanding whether a parallel program is efficient and where its bottlenecks lie. The most important metric is **speedup**: the ratio of sequential execution time to parallel execution time, $S(p) = T_1 / T_p$. **Parallel efficiency** is $E(p) = S(p) / p$, measuring how well the processors are utilized. **Super-linear speedup** ($S(p) > p$) occasionally occurs when the aggregate cache of multiple processors allows the working set to fit in cache, reducing memory access time.

The **roofline model**, introduced by **Samuel Williams**, Andrew Waterman, and David Patterson in 2009, provides a visual framework for identifying performance bottlenecks. The model plots achievable performance (in FLOPS) as a function of **operational intensity** (FLOPS per byte of data transferred from memory). The performance is bounded by two ceilings: the **peak compute rate** of the processor and the **peak memory bandwidth** multiplied by the operational intensity:

$$\text{Performance} \leq \min\!\left(\text{Peak FLOPS},\; \text{Peak Bandwidth} \times \text{Operational Intensity}\right)$$

If a program falls below the memory bandwidth ceiling, it is **memory-bound** and will benefit from optimizations that reduce data movement (cache blocking, data layout optimization, prefetching). If it falls below the compute ceiling, it is **compute-bound** and will benefit from better vectorization, algorithm improvements, or more processors.

Profiling tools provide empirical data to guide optimization. NVIDIA's **Nsight Compute** profiles GPU kernels at the instruction level, identifying issues like warp divergence, low occupancy, and uncoalesced memory accesses. Intel's **VTune** and AMD's **uProf** analyze CPU-side performance, including cache misses, branch mispredictions, and floating-point utilization. **PAPI** (Performance Application Programming Interface) provides a portable interface to hardware performance counters. **HPCToolkit** and **TAU** (Tuning and Analysis Utilities) support profiling and tracing of MPI and OpenMP programs across large clusters.

## Exascale Computing and Emerging Architectures

The pursuit of **exascale computing** — machines capable of $10^{18}$ floating-point operations per second — represents the current frontier of high-performance computing. The first exascale system, **Frontier** at Oak Ridge National Laboratory, achieved 1.1 exaFLOPS on the HPL benchmark in 2022, using AMD EPYC CPUs and AMD Instinct MI250X GPUs. Reaching exascale required overcoming challenges in power consumption (Frontier consumes roughly 20 megawatts), memory bandwidth, interconnect latency, and fault tolerance at a scale where component failures are not exceptional events but routine occurrences.

**Resilience** at exascale demands new approaches. **Checkpoint/restart** — periodically saving the application's state to persistent storage so that it can be resumed after a failure — is the traditional technique, but at exascale the time to write a checkpoint can become a significant fraction of the computation. **Algorithm-based fault tolerance** (ABFT) encodes redundancy directly into the computation (e.g., maintaining checksums of matrix rows and columns), allowing recovery without a full checkpoint. **Forward recovery** attempts to reconstruct lost data from surviving information, rather than rolling back to a previous state.

**Heterogeneous computing** — combining CPUs, GPUs, FPGAs, and domain-specific accelerators in a single system — is the architectural paradigm of the exascale era. Programming such systems requires abstractions that can express parallelism across diverse hardware. **OpenACC** provides directive-based GPU programming for Fortran and C, similar in philosophy to OpenMP. Emerging programming models like **SYCL**, **Kokkos**, and **Raja** aim to provide performance portability across different accelerator types. **Chapel** (from Cray/HPE), **Legion** (from Stanford), and **Charm++** (from UIUC) explore higher-level abstractions — global address spaces, task-based execution, and load-balancing runtimes — that can hide the complexity of heterogeneous distributed systems.

Beyond classical silicon, **quantum computing** promises exponential speedups for certain problems (integer factoring, unstructured search, quantum simulation) but remains in the noisy intermediate-scale quantum (NISQ) era, where error rates and qubit counts limit practical utility. **Neuromorphic computing**, exemplified by Intel's Loihi and IBM's TrueNorth chips, processes information using spike-based neural network models with extraordinary energy efficiency for certain pattern-recognition tasks. **In-memory computing** places processing logic within or near the memory array, attacking the data-movement bottleneck at its source. **Photonic computing** uses light for certain linear operations (matrix-vector multiplication), offering high bandwidth and low energy per operation. These emerging technologies do not replace conventional parallel computing but extend it, and the challenge of integrating them into productive, programmable systems defines the research frontier of the field.
